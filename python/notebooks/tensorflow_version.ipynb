{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        # np.random.seed(100)\n",
    "        np.random.seed(int(time.time() * 1e6) % 2 ** 31)\n",
    "        dataset = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n",
    "        self.train_x = dataset.train.images\n",
    "        self.train_x = self.train_x.reshape([-1, 28, 28, 1])\n",
    "        self.train_y = dataset.train.labels\n",
    "        self.valid_x = dataset.validation.images\n",
    "        self.valid_x = self.valid_x.reshape([-1, 28, 28, 1])\n",
    "        self.valid_y = dataset.validation.labels\n",
    "        self.test_x = dataset.test.images\n",
    "        self.test_x = self.test_x.reshape([-1, 28, 28, 1])\n",
    "        self.test_y = dataset.test.labels\n",
    "\n",
    "        train_mean = self.train_x.mean()\n",
    "        self.train_x -= train_mean\n",
    "        self.valid_x -= train_mean\n",
    "        self.test_x -= train_mean\n",
    "\n",
    "    def __get_batches_from(self, set_x, set_y, num_batches=100):\n",
    "        Xs = []\n",
    "        Ys = []\n",
    "        N = set_x.shape[0]\n",
    "        groupedIndexes = np.split(np.random.permutation(N), num_batches)\n",
    "        for group in groupedIndexes:\n",
    "            Xs.append(set_x[group])\n",
    "            Ys.append(set_y[group])\n",
    "\n",
    "        return zip(Xs, Ys)\n",
    "\n",
    "    def get_train_batches(self, num_batches=100):\n",
    "        return self.__get_batches_from(self.train_x, self.train_y, num_batches)\n",
    "\n",
    "    def get_valid_batches(self, num_batches=100):\n",
    "        return self.__get_batches_from(self.valid_x, self.valid_y, num_batches)\n",
    "\n",
    "    def get_test_batches(self, num_batches=100):\n",
    "        return self.__get_batches_from(self.test_x, self.test_y, num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        tf.reset_default_graph()\n",
    "        imageSize = 28\n",
    "        classesNum = 10\n",
    "\n",
    "        self.inputs = tf.placeholder(tf.float32, (None, imageSize, imageSize, 1))\n",
    "        self.labels = tf.placeholder(tf.float32, (None, classesNum))\n",
    "\n",
    "            net = layers.fully_connected(net, 512, scope='fc3')\n",
    "            # net = layers.fully_connected(net, 10, scope='fc4')\n",
    "\n",
    "        self.logits = layers.fully_connected(net, classesNum, activation_fn=None, scope='logits')\n",
    "        self.loss = tf.losses.softmax_cross_entropy(self.labels, self.logits) + sum(\n",
    "            tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "    def draw_conv_filters(self, epoch, step, weights, save_dir):\n",
    "        C = 1\n",
    "        w = weights.copy()\n",
    "        num_filters = w.shape[3]\n",
    "        k = w.shape[1]\n",
    "        w = w.reshape(num_filters, C, k, k)\n",
    "        w -= w.min()\n",
    "        w /= w.max()\n",
    "        border = 1\n",
    "        cols = 8\n",
    "        rows = math.ceil(num_filters / cols)\n",
    "        width = cols * k + (cols - 1) * border\n",
    "        height = rows * k + (rows - 1) * border\n",
    "        # for i in range(C):\n",
    "        for i in range(1):\n",
    "            img = np.zeros([height, width])\n",
    "            for j in range(num_filters):\n",
    "                r = int(j / cols) * (k + border)\n",
    "                c = int(j % cols) * (k + border)\n",
    "                img[r:r + k, c:c + k] = w[j, i]\n",
    "            filename = 'epoch_%02d_step_%06d_input_%03d.png' % (epoch, step, i)\n",
    "            ski.io.imsave(os.path.join(save_dir, filename), img)\n",
    "\n",
    "    def train(self, dataset, param_niter=10, num_batches=100):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(1, 1 + int(param_niter)):\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            batch_i = 0\n",
    "            for x, y in dataset.get_train_batches(num_batches):\n",
    "                loss_val, logits, _ = self.sess.run([self.loss, self.logits, self.train_step],\n",
    "                                                    feed_dict={self.inputs: x, self.labels: y})\n",
    "                train_loss += loss_val\n",
    "                train_correct += np.sum(y == (self.logits_to_hot(logits)))\n",
    "\n",
    "                batch_i += 1\n",
    "            filters = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[0].eval(self.sess)\n",
    "            self.draw_conv_filters(i, batch_i, filters, SAVE_DIR)\n",
    "\n",
    "            train_loss /= num_batches\n",
    "            train_correct /= dataset.train_y.shape[0] * dataset.train_y.shape[1]\n",
    "\n",
    "            valid_loss = 0\n",
    "            valid_correct = 0\n",
    "            for x, y in dataset.get_valid_batches(num_batches):\n",
    "                loss_val, logits = self.sess.run([self.loss, self.logits], feed_dict={self.inputs: x, self.labels: y})\n",
    "                valid_loss += loss_val\n",
    "                valid_correct += np.sum(y == (self.logits_to_hot(logits)))\n",
    "            valid_loss /= num_batches\n",
    "            valid_correct /= dataset.valid_y.shape[0] * dataset.valid_y.shape[1]\n",
    "\n",
    "            print(\"Iteration\", i, \"has loss:\", (train_loss, valid_loss))\n",
    "            print(\"Precision:\", (train_correct, valid_correct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
