\documentclass[times, utf8, numeric, diplomski]{fer}
\usepackage{booktabs}			% Tables.
\usepackage{cite}				% Citations.
\usepackage{graphicx}			% Images.
\usepackage{xcolor}				% Text color.
\usepackage[]{algorithmic}		% Algorithms.
\usepackage[]{algorithm}		% Algorithms.
\usepackage{bbm}					% Fancy font for maths.
\usepackage{multirow}			% Multirow cells in tables.
\usepackage{float}				% Image position on page.
\usepackage{amsmath}				% Maths.
\usepackage{subcaption}			% Captions for figures.
\usepackage{url}					% URLs.
\usepackage[hidelinks]{hyperref} % Hyperrefs throughout PDF.

\graphicspath{ {./img/} }
\floatname{algorithm}{Algoritam}

% Math
\def\mat#1{\underline{#1}}
\def\expect{\mathbb{E}}
\def\realnum{\mathbb{R}}
\def\pfrac#1#2{\frac{\partial #1}{\partial #2}}
\def\dfrac#1#2{\frac{d #1}{d #2}}
\def\F1{F$_1$}
\def\normal{\mathcal{N}}
\def\probsep{\ |\ }
\def\dataset{\mathcal{D}}
\def\minibatch{\mathcal{M}}
\def\otherwise{\textit{inače}}

\def\SBOX#1{\tt Sbox(#1)}
\def\SBOXi#1{\tt Sbox^{-1}(#1)}

% Referencing
\def\figref#1{(slika \ref{#1})}
\def\secref#1{(poglavlje \ref{#1})}

% TODO notes
\def\TODO#1{\noindent\textcolor{red}{TODO: \textit{#1}}\newline}
\def\todo#1{\TODO{#1}}
\def\todoimg#1{\begin{center} \textcolor{red}{\big[ IMAGE: \textit{#1} \big]} \end{center}}
\def\todoeq#1{\textcolor{red}{\begin{equation}\text{\big[EQUATION: \textit{#1}\big]}\end{equation}}}

\begin{document}

\thesisnumber{1966}

\title{Optimizirane aktivacijske funkcije klasifikatora temeljenog na umjetnim neuronskim mrežama u domeni implementacijskih napada na kriptografske uređaje}

\author{Juraj Fulir}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik

\zahvala{TODO: ZAHVALE}

\tableofcontents

%--------------------------------------------------------------------------------------%
\chapter{Uvod}
Kriptografski uređaji postali su nezamjenjiv element digitalne infrastrukture modernog društva. Služe za sakrivanje važnih informacija, poput bankovnih transakcija i autorskih djela, i ograničavanje pristupa neovlaštenim osobama. S obzirom na vrijednost informacije koju prikrivaju, česta su meta napada. Znanstvena zajednica provodi etičke napade i analize sigurnosnih sustava, ali i predlaže poboljšanja sigurnosti kriptografskih uređaja, s ciljem da se obznane njihove slabosti i na vrijeme spriječe nedetektirani napadi. Analiza emisija kriptografskog uređaja omogućava jednu vrstu napada koju je gotovo nemoguće detektirati.

Duboko učenje se pokazalo vrlo uspješnim na teškim problemima u raznim domenama primjene, pa tako i u napadima na kriptografske uređaje. No optimizacija dubokih modela je vrlo složen proces s raznim hiperparametrima čijim se ugađanjem mogu postići velike razlike u performansama. Jedan od hiperparametara je aktivacijska funkcija koja definira nelinearnost dubokog modela. Iako postoje aktivacijske funkcije s teorijski potkrijepljenim svojstvima, ne pokazuju se uvijek najboljim odabirom. Kombinacijom aktivacijskih funkcija moguće je pospješiti optimizaciju dubokih modela i povećati efektivni kapacitet modela. Stoga je istraživanje aktivacijskih funkcija vrlo važna komponenta u razvoju dubokog učenja.

Aktivacijske funkcije moguće je predstaviti stablom što otvara mogućnost optimizacije genetskim programiranjem. Iako je primjena evolucijskih algoritama u kontekstu neuroevolucije vrlo skupa zbog dugotrajnog postupka učenja, njihova primjena omogućava istraživanje optimalnih hiperparametara za postizanje boljih rezultata.

Ovaj rad se temelji na spomenutim područjima te su priloženi kratki opisi područja i elemenata korištenih u radu.

%--------------------------------------------------------------------------------------%
\chapter{Implementacijski napadi na kriptografske uređaje}

\section{Napredni standard enkripcije (AES)}
\engl{Advanced Encryption Standard}

AES je vrlo često korišten algoritam enkripcije simetričnim ključem opisan standardom \citet{aes_standard}. Ulazni podatci i izlaz iz algoritma definirani su 128-bitnim blokovima. Veličinu ključa je moguće zadati, a podržane su veličine od 128, 192 i 256 bita. Navedene veličine tipično se definiraju brojem riječi koje predstavljaju blokove od 4B (128b = 16B = 4 riječi). Prilikom izvršavanja broj iteracija algoritma ovisi o veličini ključa prema tablici \ref{tab:aes_rounds}. Na početku algoritma ulaz se kopira u matricu stanja veličine 4B$\times$4B popunjavajući stupac po stupac. Tijekom izvršavanja algoritma matrica se modificira i po završetku predstavlja izlaz algoritma.

\begin{table}[H]
\begin{tabular}{lrr}
Naziv algoritma & Veličina ključa [riječ] & Broj iteracija \\
\hline
AES-128 & 4 & 10 \\
AES-192 & 6 & 12 \\
AES-256 & 8 & 14
\end{tabular}
\centering
\caption{Ovisnost broja iteracija o veličini ključa}
\label{tab:aes_rounds}
\end{table}

AES koristi četiri operacije čijom se iterativnom primjenom s različitim potključevima ostvaruje enkripcija. Za sve operacije i za čitav algoritam definiran je inverz.
\begin{enumerate}
\item
\textbf{ZamijeniBajtove} nelinearno izmjenjuje svaki bajt matrice stanja pomoću tablice supstitucije (S-box). Svaki bajt matrice stanja predstavlja lokaciju u tablici sa koje se uzima zamjenski bajt. Tablica supstitucije je nepromjenjiva i javno dostupna.

\item
\textbf{PosmakniRetke} ciklički posmiče retke za broj mjesta koji odgovara indeksu retka (npr. trećem retku se posmiču dva bajta).

\item
\textbf{IzmješajStupce} nad svakim stupcem zasebno provodi polinomijalno modulo-množenje s konstantnim i javno poznatim polinomom. 

\item
\textbf{DodajPotključ} operatorom ekskluzivno-ili (XOR) matrici stanja dodaje se potključ stvoren ekspanzijom izvornog ključa.
\end{enumerate}

Ekspanzijom ključa se između svake dvije iteracije algoritma iz izvornog ključa stvara novi potključ. Pri prvom dodavanju potključa, ekspanzija nije primijenjena i izvorni ključ se direktno dodaje stanju. Svi naknadni potključevi generiraju se iz potključa prethodne iteracije primjenom nelinearne transformacije.

%\todoeq{Algoritam key expansion}

%Opisati: blokovi od 128 bita, ključevi 128, 192, 256, runde te četiri operacije. Napraviti neku slikicu procesa AES-a. Što je to S-kutija? Dakle, ukratko nešto reći o AES-u da bi se onda mogao opisati napad na AES programsku te sklopovsku implementaciju. DPAv2 je napad na sklopovsku implementaciju, a DPAv4 na programsku.

\section{Diferencijalna analiza potrošnje električne energije (DPA)}
\engl{Differential Power Analysis}

Moderni kriptografski uređaj je elektronički uređaj na kojem se izvodi kriptografski algoritam i koji čuva kriptografski ključ \citep{paa_book}. Izvršavanje kriptografskih algoritama na fizičkim uređajima otvara nove probleme u sigurnosti algoritama. U stvarnom svijetu područje interesa ne predstavlja samo sigurnost kriptografskog algoritma, već cijelog okruženja u kojem se algoritam izvodi. Primjeri nekih kriptografskih uređaja su pametne kartice i \textit{USB} uređaji.

Uspješan napad na kriptografski uređaj označava otkrivanje tajnog ključa pohranjenog na uređaju. Neovlaštena osoba koja pokušava saznati tajni ključ pohranjen na uređaju naziva se napadač, a pokušaj neovlaštenog otkrivanja tajnog ključa naziva se napad. Da bi se provjerila sigurnost kriptografskog uređaja, potrebno je uvesti pretpostavke o informacijama kojima napadač može pristupiti. Najjača pretpostavka koja se nadovezuje na Kerckhoffovo načelo \footnote{Kriptografski algoritam mora biti siguran i onda kada su sve informacije o njemu poznate, a nepoznat samo tajni ključ.} jest da napadač zna detalje o kriptografskom uređaju.

\section{Implementacijski napadi}
\label{sec:implatt}

Implementacijski napadi iskorištavaju slabosti fizičkog uređaja za neovlašteni pristup podatcima. Među dominantnijim su napadima na kriptografske uređaje zbog svoje povoljne cijene i učinkovitosti \citep{picekphd}. Postoje pasivni i aktivni implementacijski napadi. Pasivnim napadom generalno se ne ometa rad kriptografskog uređaja te se izvodi mjerenjima uređaja u radu. Aktivni napadi uključuju fizičku modifikaciju s ciljem izmjene predviđenog rada uređaja. Prema invazivnosti napada, implementacijski napadi dijele se na neinvazivne, polu-invazivne (pristup vanjskim dijelovima uređaja) i invazivne (pristup aktivnom sklopovlju uređaja) \citep{picekphd}. 

Napadi koji mjere sporedna svojstva kriptografskih uređaja \engl{side-channel attacks} su među najčešćim napadima. Među popularnijim su i napadi sondiranjem \engl{probing attacks} \citep{paa_book} te napadi analizom pogreški \engl{fault attacks}.

%Slika \ref {fig:sca} primjer je napada koji koristi potrošnju električne energije kao sporedno svojstvo kriptografskog uređaja.

\section{Potrošnja električne energije}
Prema podjeli u \ref{sec:implatt}, napadi koji koriste sporedna fizikalna svojstva kriptografskih uređaja (\textit{SCA}) pripadaju pasivnim, odnosno neinvazivnim napadima. Kod takvih napada iskorištavaju se saznanja o potrošnji električne energije \engl{power analysis attack, PAA}, elektromagnetskom zračenju, vremenu izvođenja ili zvuku koji proizvodi uređaj \citep{picekphd}. U nastavku je detaljnije objašnjen napad koji se zasniva na saznanju o potrošnji električne energije. Signal koji opisuje potrošnju električne energije zove se trag.

%\todoimg{Napad koji koristi sporedna fizikalna svojstva kriptografskog uređaja}

PAA iskorištava činjenicu da trenutna potrošnja električne energije uređaja ovisi o trenutnim podacima u registrima i o kriptografskim operacijama koje se izvode. PAA napadi dijele se na direktne napade i dvorazinske napade. Među direktne PAA napade spadaju: jednostavna analiza potrošnje električne energije \engl {simple power analysis, SPA}, diferencijalna analiza potrošnje električne energije \engl {differential power analysis, DPA}, korelacijska analiza potrošnje električne energije \engl {correlation power analysis, CPA} te kolizijski napadi \engl {collision attacks, CA}. Dvorazinski PAA napadi su napadi podudaranjem s obrascem \engl {template attacks}, stohastički modeli \engl {stohastic models} te linearna regresijska analiza \engl {linear regression analysis, LRA} \citep{sca_lectures}.

\subsection{Potrošnja elektroničkog sklopovlja}
Najčešće korištena tehnologija elektroničkog sklopovlja CMOS \engl{Complementary Metal–Oxide Semiconductor} koristi se za implementaciju osnovnih logičkih vrata kojima se grade složenije komponente poput registara i bistabila. Logička vrata izgrađena su od dvije komplementarne funkcije, jednom prema pozitivnom potencijalu i njenim komplementom prema negativnom potencijalu. Pri promjeni izlaza, zbog kašnjenja poluvodičkih tranzistora, privremeno nastaje kratki spoj koji se očituje skokom u potrošnji energije.

%\todoimg{NILI logička vrata implementirana pomoću CMOS tehnologije}

Ukupna disipacija električne energije uređaja je proporcionalna broju logičkih vrata koja promijene stanje. Jednadžbom \eqref{eq:pwdisipation} prikazan je spomenuti pojednostavljen model za potrošnju električne energije.

\begin{equation}
\label{eq:pwdisipation}
P\left(t\right) = \sum_{g}{} f\left(g,t\right) + N\left(t\right)
\end{equation}

\noindent gdje $f(g,t)$ predstavlja potrošnju električne energije vrata $g$ u trenutku $t$, a $N(t)$ predstavlja visokofrekventni šum zbog nesavršenosti izvora energije, generatora takta ili vanjskog utjecaja.

\subsection{Modeli emisije električne energije}
Dva poznata modela emisije električne energije \engl {power leakage model} kod SCA su model Hammingove težine \engl {Hamming weight, HW} i model Hammingove udaljenosti \engl {Hamming distance, HD}. Model Hammingove težine pretpostavlja da je potrošnja električne energije korelirana s Hammingovom težinom podataka unutar kriptografskog algoritma. Model Hammingove udaljenosti promatra broj promjenjenih bitova  prelaskom iz prethodnog stanja algoritma u trenutno stanje.

\subsection{SCA razlučitelji}
SCA razlučitelji \engl {SCA distinguishers} su statistički modeli temeljem kojih se gradi veza između mjerenja fizikalnih svojstava uređaja i tajnog ključa. Pretpostavljanjem postojanja veze između mjerenja fizikalnih svojstava kriptografskog uređaja i tajnog ključa, napadi se dijele na one koji pretpostavljaju čvrstu vezu mjerenja i tajnog ključa te one koji zaključuju na temelju statističkog modela temeljenog na mjerenjima. Razlučitelji se koriste kod napada statističkim modelom.

Za DPA često se koriste sljedeći razlučitelji: razlika srednjih vrijednosti \engl {Difference of Means, DOM}, T-test, test varijance, Pearsonova korelacija i korelacija Spearmanovog ranga \engl {Spearman's Rank Correlation} \citep{gierlichs2009empirical}. U nastavku je opisan DPA napad s razlučiteljem razlike srednjih vrijednosti.

\subsection{Opis DPA}
Postupak diferencijalne analize potrošnje električne energije zahtjeva velik broj tragova ($\approx 10^3$). U početku se grade matrica tragova i matrica hipoteza. Broj redaka obje matrice jednak je broju tragova. Broj stupaca matrice tragova jednak je broju uzoraka prikupljenih mjerenjem, a kod matrice hipoteza broj stupaca jednak je broju mogućih vrijednosti jednog okteta ključa. 

Neka selekcijska funkcija $D_\Psi(H, b)$ računa vrijednost ciljanog bita $b$ jednog okteta kriptiranog teksta $H$, pomoću 1 okteta $\Psi$ hipotetskog ključa $K$. Tragovi se zatim podijele u dvije grupe. Oni tragovi za koje vrijedi $D_\Psi(H, b) = 1$ stavljaju se u jednu grupu (\textit{$G_1$}), a za one koje vrijedi $D_\Psi(H, b) = 0$ u drugu grupu (\textit{$G_0$}). Ako je hipotetska vrijednost jednog okteta $\Psi$ tajnog ključa $K$ točna, tada će prosječna vrijednost tragova iz \textit{$G_1$} biti veća nego prosječna vrijednost tragova iz \textit{$G_0$}. Ako je vrijednost $\Psi$ bila pogrešna, tada će funkcija $D_\Psi(H, b)$ s jednakom vjerojatnošću od $p=\frac{1}{2}$ svrstavati tragove u \textit{$G_0$} ili \textit{$G_1$} i prosječni tragovi će se poklapati. Jednadžbe \ref{eq:dpaex1} i \ref{eq:dpaex2} dokazuju prethodno rečeno, a algoritmom \ref{algo:aes_dpa} opisan je postupak otkrivanja tajnog ključa u AES algoritmu pomoću diferencijalne analize potrošnje električne energije. Ovakav DPA napad, uz razlučitelj razlike srednjih vrijednosti, prvi su opisali Kocher i ostali 1999. \citep{gierlichs2009empirical}.

\begin{align}
\Delta_{D}\left[j\right] &= \frac{\sum_{i=1}^{k}D_{\Psi}\left(H_i,b\right)T_i\left[j\right]}{\sum_{i=1}^{k}D_{\Psi}\left(H_i,b\right)} - 
\frac{\sum_{i=1}^{k}\left(1-D_{\Psi}\left(H_i,b\right)\right)T_i\left[j\right]}{\sum_{i=1}^{k}\left(1-D_{\Psi}\left(H_i,b\right)\right)}, \label{eq:dpaex1}\\
\Delta_{D}\left[j\right] &\approx 2 \left( \frac{\sum_{i=1}^{k}D_{\Psi}\left(H_i,b\right)T_i\left[j\right]}{\sum_{i=1}^{k}D_{\Psi}\left(H_i,b\right)} - \frac{\sum_{i=1}^{k}T_i\left[j\right]}{k}  \right).\label{eq:dpaex2}
\end{align}

\noindent Složenost algoritma \ref{algo:aes_dpa} ovisi o broju okteta koji čine tajni ključ, najvećoj vrijednosti koju ključ može poprimiti, broju tragova te broju uzoraka koje trag sadrži.

\begin{algorithm}
	\begin{algorithmic}
		\STATE{\textbf{Ulaz:} \\ 
			$M$ -- skup blokova otvorenog teksta $\left[k \times 16\right]$, \\
			$T$ -- skup tragova $\left[k \times ts\right]$ \\
		}
		\FOR{$\forall \beta \in \left[1..16\right]$}
			\STATE{inicijaliziraj hipotetsku matricu $H\left[k \times 256\right]$}
			\STATE{inicijaliziraj matricu razlika $\Delta\left[256 \times ts\right]$}
			\FOR{$\forall \Psi \in \left[1..256\right]$}
				\STATE{$H_{\left[:, \Psi\right]} = M_{(:,\beta)} \oplus \Psi$}
				\STATE{$H_{\left[:, \Psi\right]} = SBOX(H_{\left[:, \Psi\right]})$}
				\STATE{inicijaliziraj $G_1\left[1, ts\right]$}
				\STATE{inicijaliziraj $G_2\left[1, ts\right]$}
				\FOR{$\forall \Lambda \in \left[1..k\right]$}
					\STATE{$bit = D\left(H_{\left[\Lambda, k\right]},b\right)$}
					\STATE{$G_{bit} = G_{bit} + T_{\left[\Lambda,:\right]}$}
				\ENDFOR
				\STATE{uprosječi vrijednosti $G_1$ i $G_2$}
				\STATE{$\Delta_{\left[\Psi,:\right]} = \left|G_1 - G_2\right|$}
			\ENDFOR
			\STATE{$K_{\left[1, \beta\right]} = row\left(max\left(\Delta\right)\right)$}
		\ENDFOR
		\STATE{\textbf{Izlaz:} \\
			$K$ -- tajni ključ $\left[1 \times 16\right]$
		}
	\end{algorithmic}
	\caption{Okrivanje tajnog ključa u AES kriptoalgoritmu na temelju napada diferencijalne analize potrošnje električne energije, uz korištenje razlučitelja razlike srednjih vrijednosti}
	\label{algo:aes_dpa}
\end{algorithm}

Slika \ref{fig:dpacompare} pokazuje vrijednost matrice $\Delta$ za jedan oktet $\Psi$ tajnog ključa K. Za pogođenu vrijednost $\Psi$ vidljiva je razlika između grupa te se pojavljuju vidljiva odstupanja u potrošnji energije. U suprotnom, odstupanja ne postoje što daje naslutiti da je pretpostavljena vrijednost $\Psi$ pogrešna.

\begin{figure}[H]
\centering

\begin{subfigure}{.8\textwidth}
\includegraphics[width=\textwidth]{dpacorrect.jpg}
\centering
\caption{Ispravan oktet ključa $\psi$}
\label{fig:dpacorrect}
\end{subfigure}

\begin{subfigure}{.8\textwidth}
\includegraphics[width=\textwidth]{dpawrong.jpg}
\centering
\caption{Neispravan oktet ključa $\psi$}
\label{fig:dpawrong}
\end{subfigure}

\caption{Primjer traga $T_i$ i vrijednosti u matrici $\Delta$ za ispravno \ref{fig:dpacorrect} i neispravno \ref{fig:dpawrong} pretpostavljenu vrijednost okteta $\Psi$}
\label{fig:dpacompare}
\end{figure}

Prethodno opisani postupak pripada u neprofilirane napade analizom potrošnje električne energije, u kojem napadač ne posjeduje kopiju kriptografskog uređaja na kojem bi unaprijed izgradio model otkrivanja tajnog ključa. S druge strane analiza potrošnje električne energije može se koristiti u profiliranim napadima klasifikacijom traga, gdje trag može karakterizirati Hammingovu težinu izlaza algoritma ili samu vrijednost. Ako napadač posjeduje kopiju kriptografskog uređaja, on može naučiti model koji prema potrošnji električne energije razlikuje tragove. Tako naučeni model može se zatim iskoristiti na ciljanom kriptografskom uređaju. Ovaj rad bavi se profiliranim napadom analizom potrošnje električne energije, odnosno problemom klasifikacije tragova. 

% O detaljnijem objašnjenju implementacijskih napada preporučuje se pogledati \citep{mangard2008power}. 

\section{DPAcontest skupovi podataka}
\label{sec:dpa_datasets}
DPAcontest v2 i v4 skupovi podataka izgrađeni su s ciljem standardizacije izvedbi napada na kriptografske uređaje za potrebe usporedivosti rezultata ostvarenih u znanstvenoj zajednici. Podatkovni skupovi su javno dostupni na službenoj stranici natjecanja \citep{dpa}. Podatci predstavljaju mjerenja potrošnje električne energije pri radu AES algoritma implementiranog fizički (DPAv2) ili softverski (DPAv4) i vrijednosti bajta mjerenog registra. Pojedino mjerenje se sastoji od 3253 točke, od kojih je odabrano 50 točaka koje su Pearsonovom koeficijentom najsnažnije korelirane s izlazom \citep{samiotis_dr}. Oba skupa sadrže po $10^5$ mjerenja i podijeljeni su na skupove za treniranje i testiranje u omjeru $2:1$.

% We use two datasets that mainly differ in the amount of noise and the side-channel leakage distribution -- DPAcontest v2 \citep{dpacontestv2} and DPAcontest v4 \citep{dpacontestv4}.
% We select 50 points of interests with the highest correlation between the class value and data set for all analyzed datasets and investigate scenarios with a different number of classes -- 9 classes and 256 classes.

\subsection{DPAcontest v2}
DPAcontest v2 je objavljen 2010. godine i predstavlja mjerenja potrošnje električne energije implementacije AES-128 algoritma na FPGA pločici. Signal je mjeren pri zapisivanju u registar stanja u zadnjoj iteraciji algoritma \eqref{eq:dpav2}, gdje su $C_i$ okteti enkripcije. Ulazi su vrlo šumoviti s omjerom signala naspram šuma: $SNR \in [0.0069, 0.0096]$.

\begin{equation}
\label{eq:dpav2}
Y(k^*) = \underbrace{\SBOXi{C_{b_1} \oplus k^*}}_{\text{prethodna vrijednost registra}} \oplus \underbrace{C_{b_2}}_{\text{oktet enkripcije}},
\end{equation}

Objavljene su dvije inačice skupa. Jedna inačica sadrži 256 razreda koji predstavljaju vrijednost bajta mjerenog registra. Druga inačica sadrži 9 razreda koji predstavljaju Hammingovu težinu mjerenog registra. Drugi skup je vrlo nebalansiran zbog velike razlike u entropiji informacije predstavljene središnjim klasama i rubnim klasama (težinu $0$ ili $8$ možemo dobiti samo po jednim oktetom, a težinu $4$ s čak $70$ okteta). U ovom radu se za DPAv2 koristi inačica s 256 razreda i predviđa se vrijednost okteta mjerenog registra.

\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl256_tr_instances.jpg}
\centering
\caption{PCA redukcija skupa za učenje}
\label{fig:dpa2_train_pca}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl256_ts_instances.jpg}
\centering
\caption{PCA redukcija skupa za testiranje}
\label{fig:dpa2_test_pca}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl256_tr_inputs.pdf}
\centering
\caption{Raspon i srednja vrijednost značajki}
\label{fig:dpa2_train_inputs}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl256_ts_inputs.pdf}
\centering
\caption{Raspon i srednja vrijednost značajki}
\label{fig:dpa2_test_inputs}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl256_tr_outputs.pdf}
\centering
\caption{Distribucija oznaka}
\label{fig:dpa2_train_outputs}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl256_ts_outputs.pdf}
\centering
\caption{Distribucija oznaka}
\label{fig:dpa2_test_outputs}
\end{subfigure}
\caption{Statistike podskupa za učenje i testiranje skupa DPAv2}
\end{figure}

\subsection{DPAcontest v4}
DPAcontest v4 je objavljen 2013. godine i predstavlja mjerenja potrošnje električne energije softverske implementacije maskiranog AES-128 algoritma. Maskiranje nema utjecaja jer je maska poznata, što postupak čini ekvivalentnim nemaskiranom slučaju. Signal je mjeren pri primjeni S-tablice u prvom koraku algoritma \eqref{eq:dpav4}, gdje je $P_i$ oktet ulaznog podatka. Ulazi su manje šumoviti s omjerom signala naspram šuma: $SNR \in [0.1188, 5.8577]$.

\begin{equation}
\label{eq:dpav4}
Y(k^*) = \SBOX{P_{b_1} \oplus k^{*}} \oplus \underbrace{M}_{\text{poznata maska}},
\end{equation}

Kao i za prethodni, objavljene su dvije inačice skupa. U ovom radu se za ovaj skup koristi inačica s 9 razreda (Hammingova težina). Na slikama \ref{fig:dpa4_train_outputs} i \ref{fig:dpa4_test_outputs} vidljiva je nebalansiranost skupa.

\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl9_tr_instances.jpg}
\centering
\caption{PCA redukcija skupa za učenje}
\label{fig:dpa4_train_pca}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl9_ts_instances.jpg}
\centering
\caption{PCA redukcija skupa za testiranje}
\label{fig:dpa4_test_pca}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl9_tr_inputs.pdf}
\centering
\caption{Raspon i srednja vrijednost značajki}
\label{fig:dpa4_train_inputs}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl9_ts_inputs.pdf}
\centering
\caption{Raspon i srednja vrijednost značajki}
\label{fig:dpa4_test_inputs}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl9_tr_outputs.pdf}
\centering
\caption{Distribucija oznaka}
\label{fig:dpa4_train_outputs}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{ds_nl9_ts_outputs.pdf}
\centering
\caption{Distribucija oznaka}
\label{fig:dpa4_test_outputs}
\end{subfigure}
\caption{Statistike podskupa za učenje i testiranje skupa DPAv4}
\end{figure}

\subsection{Dosadašnji rezultati}
U radu \citet{samiotis_dr} autori uspoređuju nekoliko konvolucijskih neuronskih mreža na DPAv2 i DPAv4 podatkovnim skupovima. Uspoređuju i rezultate treniranja na manjim podskupovima radi ispitivanja efikasnosti modela za manji broj uzoraka (što je sekundarni cilj DPA natjecanja). Uspoređujući rezultate mreža učenih na skupovima DPAv2 veličine 1k, 10k, 50k i 100k najbolje rezultate postižu na 10k i 100k s točnostima $0.275$ i $0.273$. Modeli poput SVM-a i linearne regresije postižu najbolje rezultate, no oni ne nadilaze nasumičan klasifikator. Na DPAv4 modeli ostvaruju najbolje rezultate na najvećim skupovima s vrijednostima $0.851$ i $0.845$. SVM postiže bolje rezultate s vrijednostima $0.955$ i $0.960$. 

Slične rezultate objavljuju \citet{picek_cnn_dpa} u kojem potpuno povezane neuronske mreže postižu rezultate usporedive konvolucijskim mrežama. Koriste modele dubine između 2-6 i širine između 10-50, s aktivacijskom funkcijom tanh ili ReLU.

%--------------------------------------------------------------------------------------%
\chapter{Klasifikator temeljen na umjetnim neuronskim mrežama}

\section{Umjetne neuronske mreže}
Umjetne neuronske mreže (nadalje „neuronske mreže“) koriste se za modeliranje višedimenzijske funkcije ili distribucije kojom se aproksimira rješenje zadanog problema iz konačnog broja primjera. Vrlo su moćan alat za savladavanje teških zadataka u raznim područjima te često dostižu ljudske performanse na zadanom problemu. Danas su vrlo raširene u raznim područjima od kojih su samo neka: računalni vid \citep{alexnet,yolo}, prirodna obrada jezika \citep{word2vec,char_cnn} i podržano učenje \citep{atari,active_learn}.

\subsection{Građa}
Neuronske mreže građene su od međusobno povezanih jedinica, tzv. neurona, modeliranih prema pojednostavljenom modelu biološkog neurona. Neuron očitava ulazne značajke sustava ili izlaze drugih neurona te ažurira svoje unutarnje stanje i stvara odziv. Utjecaj ulaza na neuron vrednuje se težinama \engl{weights} koje definiraju kako se neuron ponaša u ovisnosti o pojedinim ulazima. Aktivacijski prag neurona \engl{bias} određuje jedinstvenu osjetljivost neurona na jačinu podražaja. Težine i prag neurona zajednički se nazivaju parametrima neurona.

\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{Neuron.pdf}
\caption{Prikazani osnovni dijelovi neurona su težine ulaza ($w_i$), ulazna funkcija ($s(\vec{x},\vec{w})$) i aktivacijska funkcija ($f(x)$). Prag neurona nije prikazan zbog jednostavnosti dijagrama.}
\label{fig:neuron}
\end{figure}

Način na koji se iz ulaza gradi unutarnje stanje neurona opisan je ulaznom funkcijom. Pretvorba unutarnjeg stanja neurona u izlazni signal opisana je aktivacijskom funkcijom. Ulazna i aktivacijska funkcija opisuju ponašanje neurona \eqref{eq:neuron_functions}. Iako je ova nomenklatura dobra u većini modernih radova, ponekad razlika između ulazne i izlazne funkcije nije očigledna (kao kod radijalne bazne funkcije).

\begin{equation}
\label{eq:neuron_functions}
n(x) = (f \circ s)(x) = f(s(x))
\end{equation}

U literaturi postoje nekonzistencije oko pojma aktivacijske funkcije. U preglednim znanstvenim radovima \citep{function_survey1, function_survey2, function_survey3} pojam aktivacijske funkcije predstavlja ulaznu funkciju, što je neispravno prema brojnim današnjim radovima. Neispravno je i sa stajališta računalne neuroznanosti prema kojoj aktivacijska funkcija označava funkciju učestalosti okidanja neurona u ovisnosti o ukupnom statičkom ulazu \citep[str.~234]{neuroscience}. U starijoj literaturi pojam aktivacijske i prijenosne funkcije često se koristi ekvivalentno \citep{evolving_transfer, evo_parsimonious}. U prijedlogu standardizacije \citet{ieee_standardization} aktivacijska funkcija je definirana kao: \textit{„Algoritam za računanje vrijednosti aktivacije neurona kao funkcije njenog ukupnog ulaza. Ukupan ulaz je tipično težinska suma ulaza u neuron“}. S obzirom na nekonzistencije u literaturi i iz potrebe razlikovanja funkcija koje prikupljaju i odašilju signale iz neurona u ovom se radu koristi gore spomenuta nomenklatura (ulazna i aktivacijska funkcija).

Najpopularnije ulazne funkcije jesu afina funkcija i unakrsna korelacija.
Afina funkcija \eqref{eq:affine} je skalarni produkt vektora ulaza s vektorom težina neurona uz dodatak vrijednosti praga. Parametri neurona definiraju nagib i pomak ravnine u prostoru ulaza koja opisuje ulaz neurona. Primjenjuje se kada se ulazi u model mogu zapisati vektorom značajki čiji raspored nije bitan.
\begin{equation}
\label{eq:affine}
s(\vec{x};\mat{W},\vec{b})=\mat{W}^T \cdot \vec{x} + \vec{b}
\end{equation}

Unakrsna korelacija, za razliku od afine funkcije, koristi informaciju o susjednosti ulaznih značajki. Unakrsna korelacija vrlo je slična operatoru konvolucije, no u literaturi gotovo uvijek naziva konvolucijom. Često se koristi za ekstrakciju obrazaca u slikama \citep{alexnet}, no može se koristiti u problemima koji zahtijevaju obradu sljedova podataka. Jedan takav primjer je klasifikacija teksta pri kojoj slijed riječi gradi informaciju koja može služiti za detekciju sentimenta i ostale primjene \citep{char_cnn}.

U radu \citet{softsign} klasičnoj afinoj funkciji pribraja se L2 norma dodatne linearne funkcije čije elemente nazivaju \textit{kvadratnim filtrima}. Empirijski pokazuju da dodavanje kvadratnih filtara pospješuje performanse modela.

Povezivanjem neurona gradi se arhitektura mreže koja određuje kako podatci i gradijenti teku kroz mrežu, a time utječu na brzinu učenja i inferencije neuronske mreže. Najčešće se koriste slojevite unaprijedne arhitekture zbog jednostavnosti izvedbe. Unaprijedne arhitekture propuštaju podatke samo u jednom smjeru odnosno već izračunati neuroni se ne izračunavaju ponovno, što je posebno pogodno za optimizaciju širenjem unatrag \secref{sec:backprop}. Slojevite arhitekture omogućuju paralelizaciju izvođenja operacija na grafičkim karticama što značajno ubrzava postupke učenja i inferencije. Pri definiciji slojevite arhitekture najčešće je dovoljno navesti samo redoslijed slojeva, no ponekad je potrebno definirati i način povezivanja slojeva npr. pri uporabi preskočnih veza \citep{highwaynet, resnet, densenet}. Prvi sloj služi za postavljanje ulaza mreže i naziva se ulaznim slojem mreže. Posljednji sloj mreže služi za predikciju izlaza te mjerenje kakvoće mreže i naziva se izlaznim slojem mreže. Svi slojevi između ulaznog i izlaznog sloja nazivaju se skrivenim slojevima.

Potpuno povezana arhitektura najjednostavnija je arhitektura. Svaki neuron u potpuno povezanom sloju aktivira se pomoću svih izlaza iz prethodnog sloja. Za naučeni potpuno povezani sloj kaže se da vrši ekstrakciju značajki iz svojih ulaza. Geometrijski gledano, sloj vrši nelinearno mapiranje značajki iz dimenzije prethodnog sloja u novu dimenziju s ciljem modeliranja boljih značajki.

\subsection{Optimizacija umjetne neuronske mreže}
Optimizacijom parametara neuronska mreža prilagođava se danom zadatku, odnosno kaže se da mreža 'uči'. Optimizacija parametara najčešće se izvodi gradijentnim spustom, uz pretpostavku derivabilnosti svih komponenata neuronske mreže. Kada ta pretpostavka ne vrijedi koriste se algoritmi kombinatorne optimizacije poput algoritma roja čestica koji se spominje u \citet{skripta_nenr}. U ovom radu neuronske mreže optimiraju se gradijentnim spustom.

\subsubsection{Gradijentni spust}
\label{sec:gradijentni_spust}
Gradijentni spust je algoritam pronalaska minimuma funkcije vođen gradijentom te funkcije. Za zadanu početnu točku iterativno se pomiče u smjeru suprotnom od gradijenta funkcije u toj točki dok ne zadovolji neki od uvjeta zaustavljanja. Na strmim funkcijama gradijent je često prevelik i može izazvati oscilaciju, divergenciju ili neefikasan spust.

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{grad_descent_rate.pdf}
\caption{Prikazan je utjecaj premalog (plavo), prevelikog (crveno) i optimalnog (zeleno) koeficijenta spusta kroz dvije iteracije. Odabir koeficijenta utječe na brzinu spusta i kvalitetu konačnog rješenja.}
\label{fig:oscilira_divergira}
\end{figure}

Stoga se gradijent pri pomaku skalira koeficijentom pomaka $\mu$. Dobro odabran koeficijent pomaka može osigurati bržu konvergenciju \figref{fig:oscilira_divergira}, a kod višemodalnih funkcija i pronalazak boljeg optimuma \figref{fig:gradientni_spust}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.45]{grad_descent_modality.pdf}
\caption{Za razliku od unimodalnih, višemodalne funkcije mogu navesti gradijentni spust na nemonotoni spust.}
\label{fig:gradientni_spust}
\end{figure}

\begin{algorithm}[H]
\begin{algorithmic}
\STATE{\textbf{Ulaz:} \\ 
	$f(\vec{x})$ -- funkcija, \\
	$\vec{x}_0$ -- početna točka, \\
	$\eta$ -- koeficijent pomaka, \\
	$n$ -- broj iteracija, \\
}
\FOR{n iteracija} \STATE {
$\vec{g}_i \gets \vec{\nabla}_{\vec{x}} f(\vec{x}_i)$ \\
$\vec{x}_{i+1} \gets \vec{x}_i - \eta \cdot \vec{g}_i$ \\
} \ENDFOR
\STATE{\textbf{Izlaz:} \\
	$\vec{x}_i$ -- pronađena optimalna točka
}
\end{algorithmic}
\caption{Gradijentni spust}
\label{alg:grad_spust}
\end{algorithm}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{grad_descent_start.pdf}
\caption{Prikazan je utjecaj odabira početne točke na kvalitetu pronađenog optimuma. Loše odabrana točka (lijevo i desno) može navesti algoritam izvan poželjne regije (sredina) i zaustaviti algoritam u lokalnom optimumu.}
\label{fig:pocetna_tocka}
\end{figure}

Početna točka utječe na ishod algoritma. Kod višemodalnih funkcija s optimumima različitih kvaliteta, početna točka može definirati u koji će lokalni optimum algoritam konvergirati \figref{fig:pocetna_tocka}.

Broj iteracija definira broj pomaka od početne točke, što definira i trajanje algoritma. Generalno želi se skratiti vrijeme pretraživanja te povećati koeficijent spusta kako bi se koristilo manje pomaka. No u praksi se najčešće nailazi na višemodalne funkcije sa strmim regijama koje izazivaju oscilacije i mogu izazvati divergenciju. Stoga se češće koriste manji pomaci kroz više iteracija. Dodatno se mogu dodati modifikacije gradijenta koje nude ograničavaju veličinu gradijenta (odsijecanje gradijenta i sl.).

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{grad_descent_moment.pdf}
\caption{Prikazan je utjecaj momenta na savladavanje platoa i izbočina. Debljina kružnice predočava količinu momenta.}
\label{fig:visoravan}
\end{figure}

Problem se javlja ako algoritam odluta u visoravan na kojoj su gradijenti vrlo mali, a sama regija je s obzirom na pomake velika (slika \ref{fig:pocetna_tocka}, desno). Kad gradijent postane neupotrebljivo malen kaže se da je \textit{nestao}. U takvim slučajevima pomaže dodavanje momenta koji se akumulira kroz više iteracija i dodaje vektoru gradijenta. Kad algoritam naiđe na regiju s vrlo malim gradijentima, moment se troši i pokušava izvući algoritam iz visoravni. Kako moment ne bi skrenuo ili izvukao algoritam iz optimuma, dodaje mu se koeficijent \textit{zaboravljanja} kojim se stari vektor momenta djelomično zaboravlja u korist novog vektora pomaka (jednadžba \ref{eq:moment_update}). Moment može pomoći i pri zaobilaženju lokalnih optimuma \figref{fig:visoravan}.

\begin{equation}
\label{eq:moment_update}
\begin{split}
\vec{v} &\gets \alpha \cdot \vec{v} - \eta \cdot \vec{g} \\
\vec{x} &\gets \vec{x} + \vec{v}
\end{split}
\end{equation}

Algoritam je primjenjiv na funkcije proizvoljne dimenzionalnosti uz pretpostavku derivabilnosti u svakoj točki. Za proizvoljnu realnu funkciju, uz dobro odabrane hiperparametre, algoritam će konvergirati u jedan od lokalnih optimuma, no algoritam generalno nema garanciju konvergencije u globalni optimum. Garanciju pronalaska globalnog optimuma nudi jedino za trivijalne unimodalne funkcije uz odgovarajuće hiperparametre algoritma \figref{fig:gradientni_spust}.

Problem odabira hiperparametara proizlazi iz činjenice da u generalno praksi funkcija koja se minimizira nije definirana, već postoji samo skup primjera te funkcije, te se postupak pretrage ne može jasno vizualizirati. Čak i kad je definirana funkcija najčešće nije poznata vrijednost globalnog optimuma. Unatoč tome, gradijentni spust efikasno i učinkovito pronalazi optimume koji su dovoljno dobri za većinu praktičnih primjena \citep{alexnet, yolo}.

\subsubsection{Funkcija gubitka}
Pri učenju umjetnih neuronskih mreža potrebno je definirati funkciju gubitka. Funkcija gubitka, za dani ulaz, uspoređuje predikciju mreže sa željenim vrijednostima te dodjeljuje iznos pogreške (realan broj). Potrebno je pažljivo odabrati funkciju gubitka jer ona utječe na učenje svakog parametra \secref{sec:backprop} te definira što je ishod učenja. Najčešće nije poznata definicija funkcije gubitka na čitavoj promatranoj domeni već su poznati samo primjeri te funkcije u podatcima koji su izmjereni i koji se smatraju reprezentativnim za dani problem. Stoga se u narednim formulama umjesto integriranja koristi sumiranje.

Funkcija gubitka često je usko vezana uz vrstu problema koji se rješava (klasifikacija, regresija i ostali), način učenja (nadzirano, polu-nadzirano, nenadzirano, podržano) i aktivacijsku funkciju izlaznog sloja neuronske mreže. U ovom radu vrši se klasifikacija nadziranim učenjem, no u nastavku se navodi i primjer funkcije gubitka za regresiju. Funkciju gubitka definira se kao inverz funkcije izglednosti da neuronska mreža modelira funkciju predstavljenu primjerima podatkovnog skupa. Funkcija gubitka se minimizira pa je definirana \textbf{negativnim logaritmom izglednosti} \eqref{eq:negloglikelyhood}. Logaritam je uveden zbog pojednostavljivanja distribucija koje sadrže eksponente.

\begin{equation}
\label{eq:negloglikelyhood}
L(\theta \probsep x,y) = -\log\ p(f(X;\theta)=y \probsep X=x)
\end{equation}

Zbog jednostavnosti zapisa, izlaz modela zapisuje se skraćeno kao predikcija funkcije izlaza podatkovnog skupa te se ovisnost o ulazima i parametrima mreže podrazumijeva.
\begin{equation}
\hat{y} = f(x; \theta)
\end{equation}

S obzirom da se želi procijeniti kvaliteta modela na čitavom podatkovnom skupu gleda se očekivanje funkcije gubitka \eqref{eq:negloglikelyhood_expect}.
\begin{equation}
\label{eq:negloglikelyhood_expect}
\begin{split}
L(\theta \probsep \dataset) &= \expect_{(x,y) \sim p_\dataset}[L(\theta \probsep x,y)] \\
&= -\frac{1}{|\dataset|} \sum_{(x,y)\in\dataset} \log\ p(\hat{y}=y \probsep x)
\end{split}
\end{equation}

\textbf{U problemima regresije}, pretpostavlja se da izlazi mreže prate Gaussovu distribuciju s jediničnom kovarijacijskom matricom. Njenim uvrštavanjem u negativnu log. izglednost \eqref{eq:negloglikelyhood} dobiva se funkcija kvadratnog gubitka koja računa odstupanje izlaza neuronske mreže od željenih vrijednosti. Uz ovaj gubitak najčešće se koristi funkcija identiteta u izlaznom sloju.
\begin{align}
\label{eq:normal_dist}
\begin{split}
p(\hat{y}=y \probsep x) &= \normal(y \probsep \mu=\hat{y}, \mat{\Sigma}=\mat{I}) \\
&= \frac{1}{\sqrt{2\pi}\Sigma^{^1/_2}} \exp(-(y-\hat{y})^T\Sigma^{^1/_2}(y-\hat{y}))
\end{split}
\end{align}

Uvrštavanjem u \eqref{eq:negloglikelyhood} dobiva se kvadratni gubitak:

\begin{align}
\label{eq:loss_regression}
L(\theta \probsep x,y) &= - \log \Big[ \frac{1}{\sqrt{2\pi}\Sigma^{^1/_2}} \Big] - \log \Big[ \exp(-(y-\hat{y})^T\Sigma^{^1/_2}(y-\hat{y})) \Big] \nonumber \\
&= -\log c - (y-\hat{y})^T\Sigma^{^1/_2}(y-\hat{y}) \nonumber \\
&= \left|\footnotesize 
\begin{array}[c]{l} 
	c = konst. \\ 
	\Sigma = I 
\end{array}
\right| \nonumber \\
&= (y-\hat{y})^T(y-\hat{y}) = \sum_i (y_i - \hat{y}_i)^2
\end{align}

\textbf{U problemima klasifikacije}, pretpostavlja se da su izlazi mreže međusobno isključive slučajne varijable koje prate generaliziranu Bernoullijevu distribuciju (kategoričku distribuciju).
\begin{equation}
p(\hat{y}=y \probsep x) = \hat{y}_1^{y_1} \cdot \hat{y}_2^{y_2} \cdots \hat{y}_c^{y_c} 
= \prod_{i=1}^C \hat{y}_i^{y_i}, \quad \sum_i \hat{y}_i = 1
\end{equation}

Uvrštavanjem u \ref{eq:negloglikelyhood} dobiva se kategorički gubitak:
\begin{align}
\label{eq:loss_classification}
L(\theta \probsep x,y) &= - \log \Big[ \prod_{i=1}^C \hat{y}_i^{y_i} \Big] = - \sum_{i=1}^C y_i \log (\hat{y}_i)
\end{align}

\subsubsection{Optimizacija širenjem unatrag}
\label{sec:backprop}
Funkcija gubitka opisuje pogrešku čitave mreže te ovisi o svakom parametru mreže. Takva formulacija problema omogućuje da se svaki parametar mreže ugađa gradijentnim spustom. Dakle, za parametriziranu funkciju $f(x;\theta)$ traže se parametri $\theta^*$ za koje je gubitak najmanji na podatkovnom skupu.
\begin{equation}
\theta^* = argmin_\theta\ L(\theta \probsep \dataset), \quad \forall (x,y) \in \dataset
\end{equation}

Optimiranje gradijentnim spustom zahtjeva derivabilnost funkcije koja se optimizira po ulazima, što izrazi \eqref{eq:loss_regression} i \eqref{eq:loss_classification} zadovoljavaju. Pri tome koristi se pravilo ulančavanja parcijalne derivacije kompozicije funkcija.
\begin{equation}
\label{eq:partial_rule}
\dfrac{}{x} f(g(x)) = \pfrac{f(g(x))}{g(x)} \cdot \pfrac{g(x)}{x}
\end{equation}

Derivacijom funkcije gubitka za regresiju \eqref{eq:loss_regression} po ulazima, uz pretpostavku derivabilnosti čitave neuronske mreže po ulazima, dobiva se sljedeći izraz:
\begin{align}
\dfrac{L(\theta \probsep x,y)}{x} &= \dfrac{}{x}\sum_i(y_i - \hat{y}_i(x))^2 \nonumber \\
&= 2 \cdot \sum_i (y_i - \hat{y}_i(x)) \cdot \dfrac{\hat{y}_i(x)}{x}
\end{align}

Derivacija funkcije gubitka za klasifikaciju može se drastično pojednostaviti ako se za funkciju izlaznog sloja mreže odabere funkcija \textit{softmax} \eqref{eq:softmax_for_loss}, a izlazi se kodiraju \textit{one-hot} oznakama \eqref{eq:onehot}. Ulaz u funkciju softmax su aktivacije neurona izlaznog sloja koje su ovdje označene vektorom $\vec{s}(x)$.
\begin{equation}
\label{eq:softmax_for_loss}
\hat{y}(\vec{x}) = \text{softmax}(\vec{s}(x)) = \frac{\exp(\vec{s}(x))}{\sum_{i=1}^{C} exp(s_i(x))}
%%% \quad \equiv \quad p(y_i \probsep x) = \frac{p(x|y_i)p(y_i)}{\sum_jp(x|y_j)p(y_j)}
\end{equation}

\begin{equation}
\label{eq:onehot}
\vec{y}=[y_1, y_2, \cdots, y_C], \quad y_i \in \{0,1\}, \quad \sum_{i=1}^C y_i = 1
\end{equation}

Uvrštavanjem u \eqref{eq:loss_classification} dobivamo:

\begin{align}
\label{eq:loss_classification_softmax}
L(\theta \probsep x,y) &= - \sum_{i=1}^C y_i \log \frac{\exp(s_i(x))}{\sum_{j=1}^{C} exp(s_j(x))} \nonumber \\
&= - \sum_{i=1}^C \bigg[ y_i \cdot s_i(x) - y_i \log\big(\sum_{j=1}^{C} exp(s_j(x))\big) \bigg] \nonumber \\
&= - \sum_{i=1}^C \big[ y_i \cdot s_i(x) \big] + \log\big[\sum_{j=1}^{C} exp(s_j(x))\big] \cdot \sum_{i=1}^C y_i \nonumber \\
&= \log\big[\sum_{j=1}^{C} exp(s_j(x))\big] - \sum_{i=1}^C \big[ y_i \cdot s_i(x) \big]
\end{align}

Derivacijom \eqref{eq:loss_classification_softmax} po ulazima, koristeći pravilo \eqref{eq:partial_rule} dobivamo:
\begin{align}
\label{eq:loss_classification_softmax_deriv}
\dfrac{L(\theta \probsep x,y)}{x} &= \dfrac{L(\theta \probsep x,y)}{\vec{s}(x)} \cdot \dfrac{s(x)}{x} \nonumber \\
&= \bigg[ \vec{s}(x) - y \bigg] \cdot \dfrac{s(x)}{x}
\end{align}

S obzirom da se mreža sastoji od ulančanih nelinearnih neurona s parametrima, gradijent gubitka mora se proslijediti sekvencijalno širenjem unazad (prema ulazima u mrežu). Pojedini neuron može se smatrati parametriziranom funkcijom koju je moguće prikazati grafom. Ulazni gradijent prolaskom kroz neuron širi se na parametre i ulaze neurona koji vode do neurona koji mu prethode. Dodatno, gradijent se širi u suprotnom smjeru od toka podataka. Iz toga proizlazi naziv \textit{širenjem unazad} \engl{backpropagation}.

Želi li se mrežu učiti gradijentnim spustom, svaki parametar mreže treba imati pristup gradijentu funkcije gubitka. S obzirom da je neuron parametrizirana funkcija, pomoću koje se ulančavanjem gradi mreža, dovoljno je pokazati da pojedini neuron osigurava svojim parametrima pristup gradijentu te da gradijent šalje svojim prethodnicima s kojima je povezan. Da je to ostvarivo dokazuju izrazi:
\begin{align}
\label{eq:backprop_activation}
\begin{split}
\pfrac{L(x,y;\theta)}{s(x;w)} &= \pfrac{L(x,y;\theta)}{o(x;w)} \cdot \pfrac{o(x;w)}{s(x;w)} \\
&= \pfrac{L(x,y;\theta)}{o(x;w)} \cdot f'(x)
\end{split}
\end{align}

\begin{align}
\begin{split}
\pfrac{L(x,y;\theta)}{x_i} &= \pfrac{L(x,y;\theta)}{s(x;w)} \cdot \pfrac{s(x;w)}{x_i} \\
&= \pfrac{L(x,y;\theta)}{o(x;w)} \cdot f'(x) \cdot w_i
\end{split}
\end{align}

\begin{align} \label{eq:w_update}
\begin{split}
\pfrac{L(x,y;\theta)}{w_i} &= \pfrac{L(x,y;\theta)}{s(x;w)} \cdot \pfrac{s(x;w)}{w_i} \\
&= \pfrac{L(x,y;\theta)}{o(x;w)} \cdot f'(x) \cdot x_i
\end{split}
\end{align}

\begin{align}
\begin{split}
\pfrac{L(x,y;\theta)}{w_0} &= \pfrac{L(x,y;\theta)}{s(x;w)} \cdot \pfrac{s(x;w)}{w_0} \\
&= \pfrac{L(x,y;\theta)}{o(x;w)} \cdot f'(x) \cdot 1
\end{split}
\end{align}

\subsubsection{Stohastički gradijentni spust}
U prošlom poglavlju korištena je funkcija gubitka na jednom podatkovnom paru, no želi se minimizirati očekivanje funkcije gubitka \eqref{eq:negloglikelyhood_expect} na cijelom podatkovnom skupu jer se želi izgraditi model koji aproksimira čitavu uzorkovanu funkciju što bolje. Nažalost, podatkovni skupovi poput spomenutog u poglavlju \ref{sec:dpa_datasets} vrlo su veliki i nije ih praktično koristiti pri optimizaciji gradijentom jer bi se čitavi morali držati u brzoj memoriji. Ovo predstavlja velik problem pri računanju s grafičkim karticama. Očekivanje gubitka služi za procjenu smjera i veličine gradijenta. Ako se gradijent procjenjuje na čitavom podatkovnom skupu on će nas dovesti do prvog optimuma koji najčešće nije dobar, odnosno snažno ovisi o odabiru početne točke. 

S druge strane, ako se gradijent računa na pojedinom podatkovnom paru kao u \eqref{eq:loss_classification} ostvareno je stohastičko kretanje jer će svaki primjer usmjeriti gradijent u drugom smjeru. Posljedica toga je duže vrijeme pretrage, no veća otpornost na loše lokalne optimume. Iako će algoritam generalno konvergirati, postupak je zahtjevan jer za svaki primjer podatkovnog skupa treba provesti korak algoritma. Ovaj problem također ograničava svojstvo ubrzanja grafičkim karticama zbog stalne potrebe da dohvatom podataka preko sabirnice.

\begin{algorithm}[H]
\begin{algorithmic}
\STATE{\textbf{Ulaz:} \\
	$f(\vec{x})$ -- funkcija, \\
	$\vec{\theta}_0$ -- početni parametar, \\
	$\eta$ -- stopa učenja, \\
	$k$ -- kriterij zaustavljanja
}
\WHILE{kriterij $k$ nije zadovoljen}
	\FOR{$\minibatch_i \in \dataset$} \STATE {
		$\vec{g}_i \gets \frac{1}{|\minibatch|} \sum_{(\vec{x},\vec{y})\in\minibatch} \vec{\nabla}_{\theta} L(\theta \probsep \vec{x},\vec{y})$ \\
		$\vec{\theta}_{i+1} \gets \vec{\theta}_i - \eta \cdot \vec{g_i}$
	 } \ENDFOR
\ENDWHILE
\STATE{\textbf{Izlaz:} \\
	$\vec{\theta}^*$ -- pronađeni optimalni parametar
}
\end{algorithmic}
\caption{Stohastički gradijentni spust mini-grupama}
\label{alg:sgd}
\end{algorithm}

\paragraph{}
Iz navedenih razloga bolje je vršiti procjenu gradijenta na "nekoliko" primjera. Podskup takvih primjera naziva se \textbf{mini-grupa} \engl{mini-batch}. Najčešće se uzima najveći broj primjera koji grafička kartica može efikasno obraditi i koji je potencija broja 2. Mini-grupe i dalje zadržavaju stohastičnost kretanja gradijenta (posebice ako je veličina manja od broja klasa), ali ubrzavaju konvergenciju naspram istinski stohastičkog spusta. Sada formula \eqref{eq:negloglikelyhood_expect_minibatch} poprima oblik koji odgovara očekivanju pojedine mini-grupe $\minibatch_i$.
\begin{equation}
\label{eq:negloglikelyhood_expect_minibatch}
\begin{split}
L(\theta \probsep \dataset) &= \expect_{(x,y) \sim p_{\minibatch_i}}[L(\theta \probsep x,y)] \\
&= -\frac{1}{|\minibatch_i|} \sum_{(x,y)\in\minibatch_i} \log\ p(\hat{y}=y \probsep x)
\end{split}
\end{equation}

Mini-grupe se najčešće definiraju prije početka učenja nasumičnim rasporedom. To je najjednostavniji i najbrži pristup, no definiranjem fiksnih mini-grupa unosi se induktivna pristranost jer nije moguće znati koji je raspored idealan za optimizacijski postupak. Pristranost se može ublažiti miješanjem podatkovnog skupa između epoha, no skupocjenost te operacije ograničava njeno korištenje u praksi. Također, problem mogu stvoriti mini-grupe pristrane jednoj ili više dominantnih (većinskih) klasa koje će snažno usmjeravati gradijent prema tim klasama i zanemarivati ostale. Taj problem je posebno izražen u nebalansiranim podatkovnim skupovima, a može se zaobići tehnikom težinskog uzorkovanja s ponavljanjem gdje je vjerojatnost odabira primjera obrnuto proporcionalna dominantnosti (brojnosti) njegove klase.

\subsubsection{Optimizator}
Optimizator brine o pomaku odnosno ažuriranju parametara modela pri gradijentnom spustu. Dosad su spomenuta dva pristupa: klasični s koeficijentom pomaka (algoritam \ref{alg:grad_spust}) i s dodatkom momenta \eqref{eq:moment_update}.

Optimizator Adam \engl{adaptive moments} dodatno prati povijest momenta drugog reda kojom prilagođava stopu učenja kako bi smanjio razlike između parametara koji su povijesno dobivali velike ili male gradijente, čime se izglađuje prostor pretrage parametara. Povijest drugog momenta prate AdaGrad i RMSProp, no Adam dodatno prati povijest prvog momenta i uključuje centriranje momenata. Adam se pokazao vrlo robusnim optimizatorom i vrlo često se koristi za optimizaciju dubokih neuronskih mreža. \citep[poglavlje~8.5]{goodfellowbook}

\subsubsection{Promjenjiva stopa učenja}
Tijekom optimizacije gradijentnim spustom uz fiksnu stupu učenja optimizacija će početi oscilirati oko iste vrijednosti, odnosno srednja vrijednost gubitka će konvergirati. Oscilacija se javlja jer je stopa učenja suviše velika za regije prostora prema kojima vode gradijenti mini-grupa. U takvim situacijama potrebno je smanjiti stopu učenja, no zbog stohastične prirode spusta teško je detektirati takve situacije. Optimizator Adam prilagođava gradijente skaliranjem, što se može interpretirati kao prilagođavanje stope učenja. Unatoč tome, ponekad je potrebno koristiti dodatne mehanizme smanjivanja stope učenja.

%Jedan način smanjivanja stope je zadavanje ključnih iteracija u kojima se prestaje koristiti stara stopa i koristi se nova. Moguće je zadati stope za svaku od zadanih ključnih iteracija %\citep{onaj rad} ili zadati početnu stopu koja se u ključnoj iteraciji pomnoži konstantom manjom od 1. Pri ovom pristupu problem je odrediti optimalne ključne iteracije ili početnu iteraciju i konstantu.
%\todo{fali tekst (nađi onaj rad)}

%Ključnim iteracijama mogu se proglasiti one u kojima je detektirana konvergencija ili divergencija. Taj pristup omogućava nastavak pretrage, no problem je kako sa sigurnošću detektirati konvergenciju ili divergenciju. Tipično učenje mreže nije monotono, tj. gubitak mreže može na neko vrijeme rasti prije no što nastavi padati, što odgovara ponašanju u lokalnom optimumu. U tom slučaju smanjivanje stope učenja je nepoželjno.

Pristup koji se koristi u ovom radu je zadanu početnu stopu u svakoj iteraciji pomnožiti s konstantom manjom od 1, odnosno stopa se konstantno smanjuje. Uz stohastičan spust, postupak ostvaruje efekt sličan simuliranom kaljenju gdje se smanjivanjem temperature sustava smanjuje njegova stohastičnost i postupak konvergira u dobre regije rješenja. Smanjivanje stope učenja pri stohastičkom gradijentnom spustu smanjuje varijancu gradijenata te omogućava stabilnije i preciznije učenje. U radu se koristi eksponencijalno opadanje stope učenja po epohama. Prije svake epohe, stopa učenja se izračuna prema formuli \eqref{eq:learning_rate_decay}, gdje je $e$ indeks epohe. Za $30$ iteracija, vrijednost opada na $73\%$, a za $40$ iteracija na $67\%$ početne stope učenja. Iako bi u teoriji Adam trebao biti dovoljan, preliminarni eksperimenti pokazuju da dodani mehanizam pomaže pri učenju mreže.
\begin{equation}
\label{eq:learning_rate_decay}
\eta_e = \eta_0 \cdot 0.99^e,\quad e \in [0, N]
\end{equation}

\subsubsection{Inicijalizacija parametara}
Inicijalizacija parametara mreže odgovara odabiru početne točke pri gradijentnom spustu. Dobrom inicijalizacijom može se osigurati pronalazak boljeg optimuma te u kritičnim slučajevima i osigurati konvergencija mreže. S obzirom da neuronske mreže posjeduju jasnu strukturu (što je posebno izraženo u slojevitim arhitekturama) inicijalizacija ima dodatne utjecaje. U potpuno povezanim slojevima želi se postići da svaki neuron vrši ekstrakciju značajki. Pri tome, u efikasnom sloju svaki će neuron stvoriti jedinstvenu značajku. Ako je više neurona inicijalizirano dovoljno slično, ti će neuroni graditi iste značajke što se naziva \textbf{koadaptacijom neurona}.

Inicijalizacija slojevitim nenadziranim učenjem ograničenog Boltzmann-ovog stroja \engl{restricted Boltzmann machine} pokazuje se dobrom u praksi \citep{relu6}. Postupak nenadzirano trenira stohastičku mrežu sloj po sloj na ulaznim značajkama kako bi slojevi ostvarili dobru ekstrakciju složenijih značajki. Nakon treniranja, mreži se dodaje izlazni klasifikacijski sloj i mreža se nadzirano ugađa na skupu za učenje. Iako je ostvarena inicijalizacija vrlo dobra, postupak je vremenski zahtjevan. 

%\todo{postoji i init. autoenkoderom, ali u kontekstu evolucije je preskupa}

U ovom radu koristi se Xavier inicijalizacija koja parametre uzorkuje iz centrirane normalne distribucije s varijancom koja ovisi o broju ulaznih i izlaznih neurona. Time se postiže normalizacija ulaza u aktivacijsku funkciju i potiče korištenje njene nelinearnosti oko ishodišta. \citep{xavier}
\begin{equation}
W_{i,j} \sim \normal(0, \frac{2}{n_{ulaza}+n_{izlaza}})
\end{equation}

\subsubsection{Normalizacija značajki}
Podatci najčešće nisu uzorkovani iz normalne distribucije. To stvara problem jer prisiljava mrežu da uči srednju vrijednost i varijancu podataka što usporava učenje i stvara numeričke probleme. Stoga je korisno prije korištenja svesti značajke na normalnu distribuciju sa sredinom $0$ i varijancom $1$. To se izvodi računanjem koeficijenata distribucije te oduzimanjem sredine i dijeljenjem s varijancom.

Koeficijenti se računaju iz značajki podatkovnog skupa za učenje (\ref{eq:feat_norm_mean}, \ref{eq:feat_norm_var}). Koeficijent varijance ovisi o koeficijentu sredine pa se njihovo računanje ne može paralelizirati, no to ne predstavlja velik problem s obzirom da se računaju samo jednim prolazom kroz skup. Prije no što se značajke predaju modelu normaliziraju se formulom \eqref{eq:feat_norm}.
\begin{align}
\label{eq:feat_norm_mean}
&\vec{\mu} = \frac{1}{N} \sum_{\vec{x}\in \dataset} \vec{x} \\
\label{eq:feat_norm_var}
&\vec{\sigma^2} = \frac{1}{N-1} \sum_{\vec{x}\in \dataset} (\vec{x}-\vec{\mu})^2 \\
\label{eq:feat_norm}
&\vec{z} = \frac{\vec{x} - \vec{\mu}}{\vec{\sigma^2}}
\end{align}

Koeficijenti normalizacije računaju se isključivo na skupu za učenje, a normalizacija se mora primijeniti prilikom inferencije odnosno na skupu za testiranje.

\subsubsection{Normalizacija nad grupom}
\engl{batch normalization}

Pri optimizaciji neuronskih mreža u jednoj iteraciji se optimiziraju svi parametri. Problem pri tome je što predikcija optimizatora za parametre jednog sloja pretpostavlja da će ostali slojevi ostati netaknuti. To uzrokuje promjenu srednje vrijednosti i varijance odziva koja se akumulira prolaskom kroz mrežu. Iz tog razloga koristi se normalizacija grupom koja standardizira odzive neurona po svakoj mini-grupi. Kako to može smanjiti ekspresivnost neurona, koristi se naknadna afina transformacija s učećim parametrima.
\begin{align}
\mu &= \frac{1}{N} \sum_{i=1}^N h_i \qquad
\sigma^2 = \frac{1}{N} \sum_{i=1}^N (h_i-\mu)^2 \\
\hat{h}_i &= \frac{h_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \qquad
y_i = \gamma \cdot h_i + \beta
\end{align}

\subsection{Regularizacija}
\label{sec:regularizacija}
Decizijska granica opisuje točke u prostoru značajki za koje model dvoji između dviju ili više klasa. Geometrijski, decizijska granica može se interpretirati kao presjek dviju ili više ploha u hiperprostoru. Pri optimizaciji model prilagođava decizijsku granicu kako bi što bolje razdvojio primjere iz podatkovnog skupa koji su različitih klasa, a obuhvatio primjere iste klase. Podatkovni skupovi najčešće sadrže šum zbog nesavršenog uzorkovanja stvarne funkcije, pogrešnog označavanja ili višeznačnosti primjera. Bez regularizacije, model će s ciljem minimiziranja gubitka svoju decizijsku granicu saviti kako bi što ispravnije obuhvatio sve primjere pa čak i šum. Tada se kaže da je model počeo učiti šum odnosno da je \textbf{prenaučen}. Na prenaučenost su posebno osjetljivi složeni modeli kao što su neuronske mreže.

Kako bi se ublažila prenaučenost modela koriste se tehnike regularizacije. Pri tome se ne smije pretjerati jer suviše snažna regularizacija može ograničiti sposobnost učenja te model može postati \textbf{podnaučen}. Stoga je potrebno pronaći dobar omjer između složenosti modela i jačine regularizacije kako bi dobiveni model dobro \textbf{generalizirao}. Pri uporabi neuronskih mreža obično se odabire vrlo složen model na koji se primjenjuju razne tehnike regularizacije.

\subsubsection{Regularizacija parametara}
Prenaučenost je često posljedica velikih normi vektora parametara. Stoga se u funkciju gubitka dodaje regularizacijski član po težinama $\Omega(\theta)$. Utjecaj regularizacije moguće je mijenjati hiperparametrom $\alpha$.
\begin{equation}
\label{eq:weights_regularization}
\tilde{L}(\theta \probsep x,y) = L(\theta \probsep x,y) + \alpha\Omega(\theta)
\end{equation}

Najčešće se koristi regularizacija \textbf{L2} normom koja akumulira kvadrate svih parametara modela. Pri ažuriranju parametra formula dobiva skaliranu vrijednost tog parametra što smanjuje vrijednost negativnog gradijenta.
\begin{equation}
\Omega(\vec{\theta}) = \frac{1}{2} \|\vec{w}\|_2^2 = \frac{1}{2} \sum_i w_i^2
\end{equation}
\begin{equation}
\vec{\theta}_{i+1} \gets \vec{\theta}_i - \eta \cdot \vec{g}_i + \alpha \cdot \theta
\end{equation}

%\todo{*Spomeni pokoju još (L1, adversarial iz Hintona, ...)}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Regularizacija šumom}
Unošenjem šuma u podatke netom prije nego ih dobije model moguće je graditi robusnost modela na šum, ali ujedno to sprječava model da se pretrenira na podatkovnom skupu za treniranje. Jedan primjer može se nasumično zašumiti nekoliko puta te se time povećava efektivna veličina podatkovnog skupa.

U ovom se radu ne koristi regularizacija šumom jer bi mogla imati nepredvidivi utjecaj na učenje. Primjena na slike je vrlo jasna jer uz dovoljno malu amplitudu aditivnog šuma teksture na slici će ostati očuvane zahvaljujući susjedstvu piksela, odnosno izrezivanjem slike obrasci će ostati netaknuti. Značajke podatkovnih skupova navedenih u poglavlju \ref{sec:dpa_datasets} su PCA redukcije izvornih sljedova signala koji su ispunjeni šumom očitavanja.

\todo{ajd svejedno isprobaj, grafi za ljubav}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Rano zaustavljanje}
Pri učenju modela funkcija gubitka na skupu za učenje i testiranje generalno opada, no u jednom trenu gubitak generalizacije počinje rasti. Zaustavi li se učenje u trenutku kada je gubitak generalizacije najmanji dobiva se optimalan model za zadane hiperparametre. U praksi te funkcije neće biti naročito glatke zbog stohastičkog gradijentnog spusta te će greška generalizacije povremeno i rasti kako savladava lokalne optimume.

Postupak ranog zaustavljanja tretira broj epoha kao hiperparametar koji se pretražuje po liniji. No umjesto da se za svaku vrijednost broja epoha nanovo trenira model, ovdje se trenira jednom i odabire vrijednost za koju model generalizira najbolje. Pri mjerenju generalizacije modela koristi se unakrsna validacija \secref{sec:crossval}.

\begin{algorithm}[H]
\begin{algorithmic}
\STATE{\textbf{Ulaz:} \\
	$m$ -- model, \\
	$I$ -- maksimalan broj epoha, \\
	$\delta$ -- relativna razlika za konvergenciju, \\
	$T$ -- granica nestrpljivosti (po broju epoha) \\
}
inicijaliziraj nestrpljivost $t \gets 0$ \\
\WHILE{($++i < I\ \wedge\ t < T$)} \STATE {
	uči model jednu epohu $m$ \\
	izmjeri gubitak treniranja $L_{train}$ \\
	izmjeri gubitak validacije $L_{validate}$ \\
	\IF{$L_{train} \in \{NaN, \pm\infty\}$} \STATE {
		izađi iz petlje
	} \ELSIF{$(|\frac{L_{train}-L^*_{train}}{L^*_{train}}| \le \delta) \vee (L_{train} \ge L^*_{train}) \vee (L_{validate} \ge L^*_{validate})$} \STATE {
		$t++$
	} \ENDIF \\
	$t = 0$
	\IF{$L_{validate}<L^*_{validate}$} \STATE {
		$i^* = i$
	} \ENDIF \\
	$L^*_{train} = max\{L^*_{train},L_{train}\}$ \\
	$L^*_{validate} = max\{L^*_{validate},L_{validate}\}$ \\
} \ENDWHILE
\STATE{\textbf{Izlaz:} \\
	$i^*$ -- pronađen optimalan broj iteracija
}
\end{algorithmic}
\caption{Algoritam ranog zaustavljanja s dodatnim mehanizmima korišten u radu}
\label{alg:nested_cv}
\end{algorithm}

U ovom radu detekcija pretreniranosti radi se usporedbom s najboljom pronađenom vrijednosti funkcije gubitka na skupu za validaciju. Ako vrijednost gubitka raste $n$ uzastopnih epoha, to je pokazatelj da se mreža počela pretrenirati. Kao dodatan mehanizam ranijeg zaustavljanja, koji služi za uštedu na vremenu provedenom učenjem, koriste se detekcije divergencije i konvergencije. Divergencija je detektirana ako vrijednost gubitka na skupu za učenje raste, a konvergencija ako se taj gubitak mijenja za manje od $p\%$ najboljeg pronađenog gubitka na skupu za učenje kroz $n$ uzastopnih epoha. Sva tri mehanizma doprinose brojaču epoha i kada brojač dostigne vrijednost $n$ učenje se prekida i odabire se epoha s minimalnim gubitkom na skupu za validaciju. Ako ni jedan mehanizam nije aktiviran, brojač se ponovno postavi na $0$.

\subsection{Odabir hiperparametara}
Do ovdje su navedeni hiperparametri koji se koriste pri spomenutim tehnikama optimizacije neuronske mreže (poglavlja \ref{sec:gradijentni_spust} - \ref{sec:regularizacija}). No neuronska mreža ima i strukturalne hiperparametre.

Arhitektura mreže je vrlo bitan hiperparametar koji određuje složenost modela te utječe na brzinu inferencije i učenja modela. Razvijene su razne arhitekture koje koriste preskočne veze za postizanje vrlo dubokih arhitektura \citep{resnet, densenet}. Preskočne veze omogućavaju direktniji prijenos gradijenta što pomaže kod problema nestajućeg gradijenta u dubokim mrežama \secref{sec:nestajući_grad}. Arhitektura može omogućiti dodatnu paralelizaciju inferencije i učenja tako da se teške operacije raspodijele na više uređaja, a rezultati spoje samo kada je to nužno \citep{alexnet}.

Aktivacijske funkcije su također važan hiperparametar, no u praksi se većinom ignoriraju zbog manjka intuicije o njihovom utjecaju na učenje pojedinog modela na pojedinom podatkovnom skupu. U praksi se najčešće odabiru funkcije koje su brze i koje se pokazuju korisnima u raznim radovima. Najčešće se koristi ReLU opisan u poglavlju \ref{func:relu}. U povratnim neuronskim mrežama za rekurzivne slojeve popularan je tangens hiperbolni opisan u poglavlju \ref{func:tanh}. U poglavlju \ref{sec:aktivacijske_fje} navedene su i opisane brojne funkcije te je njihov utjecaj ispitan u poglavlju \ref{sec:rezultati}.

\subsubsection{Procjena generalizacije i odabir modela}
\label{sec:crossval}
Skup podataka kojim se uči model najčešće ne opisuje stvarnu funkciju potpuno, već sadrži primjere koji se smatraju reprezentativnim i koji su dovoljni za njeno modeliranje. Kako bi se procijenilo koliko dobro naš model procjenjuje stvarnu funkciju model se ispituje na podatcima koji nisu korišteni prilikom učenja, odnosno na neviđenim podatcima. Ta se metoda zove \textbf{unakrsna validacija} \engl{crossvalidation}, a skupovi se nazivaju \textbf{skupom za učenje} \engl{train set} i \textbf{skupom za testiranje} \engl{test set}. Dakako, važno je pobrinuti se da su oba skupa reprezentativna stvarnoj funkciji, ali da ne sadrže iste primjere. U suprotnom mreža će naučiti pogrešnu funkciju što može dati lažno pesimistične rezultate ili će se nepotpuno ili pristrano provesti testiranje što može dovesti do lažno optimističnih rezultata. Postoji više postupaka mjerenja generalizacije modela (k-preklopa, LOOCV, ...) koji osiguravaju da je generalizacija ispitana na svim primjerima, no zbog postojanja službenog skupa za testiranje i zahtjevnosti ostalih postupaka mjerenja u ovom se radu koristi \textbf{metoda izdvajanja} \engl{holdout}.

Pri odabiru hiperparametara ili pri odabiru modela potrebno je usporediti dobrote njihovih rezultata. Pri tome se skup za učenje ponovno podijeli na skup za učenje i \textbf{skup za validaciju} \engl{validation set}. Skupom za učenje model se optimizira, a skupom za validaciju ispituje se generalizaciju modela. Između kombinacija hiperparametara odabire se ona za koju model najbolje generalizira na skupu za validaciju te se njome nanovo optimizira model na izvornom skupu za učenje i dobiva konačna mjera generalizacije na skupu za testiranje. Navedeni postupak odgovara algoritmu \textbf{ugniježđene unakrsne provjere} \engl{nested crossvalidation}. U ovom radu se umjesto unakrsne validacije k-preklopa koristi metoda izdvajanja u unutarnjoj i vanjskoj petlji, a postupak je opisan pseudokodom \ref{alg:nested_cv}.

\begin{algorithm}[H]
\begin{algorithmic}
\STATE {\textbf{Ulaz:} \\
	$m$ -- model, \\
	$\dataset$ -- podatkovni skup, \\
	$\mathcal{K}$ -- kombinacije vrijednosti hiperparametara \\
}
Podijeli $\dataset$ na $\dataset_{train}$ i $\dataset_{test}$ \\
Podijeli $\dataset_{train}$ na $\dataset_{train'}$ i $\dataset_{val}$ \\
\FOR{svaku kombinaciju $k_i \in \mathcal{K}$} \STATE {
	Inicijaliziraj $m$ \\
	Optimiziraj $m$ hiperparametrima $k_i$ na $\dataset_{train'}$ \\
	Izmjeri generalizaciju $g_i$ na $\dataset_{val}$ \\
} \ENDFOR \\
$k^* \gets argmax_{g_i} k_i$ \\
Inicijaliziraj $m$ \\
Optimiziraj $m$ hiperparametrima $k^*$ na $\dataset_{train}$ \\
Izmjeri konačnu generalizaciju $g$ na $\dataset_{test}$ \\
\STATE{\textbf{Izlaz:} \\
	$m^*$ -- pronađeni optimalni model
}
\end{algorithmic}
\caption{Ugniježđena unakrsna provjera}
\label{alg:nested_cv}
\end{algorithm}

\subsubsection{Mjere generalizacije}

Za različite vrste problema koriste se različite mjere. Iako je ukupna vrijednost funkcije gubitka na čitavom skupu za testiranje dobar pokazatelj, za probleme klasifikacije koriste se mjere koje detaljnije opisuju stvarne performanse modela.

\begin{equation}
\label{eq:mat_zabune_bin}
\begin{tabular}{|c|c|c|} \hline
\large{$_{\hat{y}} \setminus ^y$} & $\top$ & $\bot$ \\ \hline
$\top$	& TP & FP \\ \hline
$\bot$	& FN & TN \\ \hline
\end{tabular}
\end{equation}

Pri \textbf{binarnoj klasifikaciji} definirana je \textbf{matrica zabune} \eqref{eq:mat_zabune_bin} koja sadrži četiri elementa \eqref{eq:true_groups_bin} koja definiraju vrstu pogotka i pogreške.
\begin{align}
\label{eq:true_groups_bin}
\begin{split}
\text{Stvarno pozitivni:} \quad TP &= \sum_{(x,y)\in\mathbb{D}} \mathbbm{1} \{h(x) = \top \wedge y = \top \} \\
\text{Stvarno negativni:} \quad TN &= \sum_{(x,y)\in\mathbb{D}} \mathbbm{1} \{h(x) = \bot \wedge y = \bot \} \\
\text{Lažno pozitivni:}   \quad FP &= \sum_{(x,y)\in\mathbb{D}} \mathbbm{1} \{h(x) = \top \wedge y = \bot \} \\
\text{Lažno negativni:}   \quad FN &= \sum_{(x,y)\in\mathbb{D}} \mathbbm{1} \{h(x) = \bot \wedge y = \top \}
\end{split}
\end{align}

Iz tih skupova se tada grade složenije mjere. \textbf{Točnost} je mjera kojom se iskazuje postotak točno pogođenih primjera:
\begin{equation}
Acc = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

Točnost je dobra mjera, no samo ako je brojnost klasa u podatkovnom skupu balansiran. Ako je brojnost jedne klase puno veća od druge tada će trivijalan klasifikator, koji sve primjere klasificira u tu klasu, davati veliku točnost i razlika naspram ispravnijeg klasifikatora bit će nezamjetna. Stoga se kod nebalansiranih setova češće koristi \textbf{\F1 mjera}, koja uzima u obzir \textbf{preciznost} klasifikatora u razlikovanju pozitivnih primjera od negativnih \eqref{eq:precision} i njegov \textbf{odziv} odnosno obuhvat svih pozitivnih primjera testnog skupa \eqref{eq:recall}. \F1 mjera je definirana kao harmonijska sredina između preciznosti i odziva \eqref{eq:f_measure}. Postoji i generalizirana mjera F$_\beta$ koja dodjeljuje veću težinu preciznosti ili odzivu \eqref{eq:f_beta}, no u ovom radu koristi se samo \F1 koja pridjeljuje jednaku težinu. Harmonijska sredina se koristi jer je najstroža između Pitagorinih mjera za sredinu kao što prokazuje slika \ref{fig:sredine}.
\begin{align}
\text{Preciznost:} \quad P &= \frac{TP}{TP+FP} \label{eq:precision} \\
\text{Odziv:} \quad R &= \frac{TP}{TP+FN} \label{eq:recall} \\
\text{F}_1: \quad F_1 &= 2 \cdot \frac{P \cdot R}{P + R} \label{eq:f_measure} \\
\text{F}_\beta: \quad F_\beta &= (1 + \beta^2) \cdot \frac{P \cdot R}{\beta ^2 \cdot P + R} \label{eq:f_beta}
\end{align}

\begin{figure}[h]
\includegraphics[width=.9\textwidth]{Pitagorine_sredine.pdf}
\centering
\caption{Pitagorine mjere za sredinu između dviju vrijednosti. Po apscisi su brojevi koji se uspoređuju s brojem $5$, a po ordinati vrijednosti mjere.}
\label{fig:sredine}
\end{figure}

Mjere binarne klasifikacije mogu se primijeniti pri \textbf{višeklasnoj klasifikaciji}, no matrica zabune je dimenzija $C\times C$ gdje je $C$ broj klasa. Elementi matrice računaju se slično kao i kod binarne klasifikacije, ali za svaku klasu posebno.
\begin{align}
\label{eq:true_groups}
\begin{split}
\text{Stvarno pozitivni:} \quad TP_i &= \sum_{(x,y)\in\mathbb{D}} \mathbbm{1} \{h(x) =  C_i \wedge y = C_i \} \\
\text{Stvarno negativni:} \quad TN_i &= \sum_{(x,y)\in\mathbb{D}} \mathbbm{1} \{h(x) \neq C_i \wedge y \neq C_i \} \\
\text{Lažno pozitivni:} \quad FP_i &= \sum_{(x,y)\in\mathbb{D}} \mathbbm{1} \{h(x) = C_i \wedge y \neq C_i \} \\
\text{Lažno negativni:} \quad FN_i &= \sum_{(x,y)\in\mathbb{D}} \mathbbm{1} \{h(x) \neq C_i \wedge y = C_i \}
\end{split}
\end{align}

Za izračun složenijih mjera poput točnosti i \F1 mjere mora se računati prosjek po klasama. Koriste se dva pristupa računanju prosjeka: makro i mikro. \textbf{Makro prosjekom} prvo se izračunaju mjere svake klase naspram svih ostalih te se uzme njihov prosjek. Ova mjera pretpostavlja jednak utjecaj svih klasa nez obzira na njihovu veličinu \citep{ml_probabilistic}.
\begin{equation}
\begin{split}
Acc^M = \sum_{i=1}^K \frac{Acc_i}{K}
\end{split} \quad
\begin{split}
P^M = \sum_{i=1}^K \frac{P_i}{K}
\end{split} \quad
\begin{split}
R^M = \sum_{i=1}^K \frac{R_i}{K}
\end{split} \quad
\begin{split}
F_1^M = \sum_{i=1}^K \frac{F_{1;i}}{K}
\end{split}
\end{equation}

\textbf{Mikro prosjekom} prvo se zbroje matrice zabune po pojedinim klasama, a zatim se nad zbrojenom matricom računaju mjere. Na mikro prosjek više utječe veličina klasa i koristi se u nebalansiranim skupovima.

\begin{align}
\begin{split}
Acc^\mu &= \frac{\sum_i (TP_i+TN_i)}{\sum_i (TP_i+TN_i+FP_i+FN_i)} = Acc^M \\
FP = FN \implies P^\mu &= R^\mu = F_1^\mu = \frac{\sum_i TP_i}{\sum_i (TP_i+FP_i)}
\end{split}
\end{align}

\subsubsection{Pretraživanje po rešetci}
\label{sec:grid_search}
Najjednostavniji način za pretraživanje hiperparametara je pretraživanje po rešetci. Za svaki hiperparametar koji se optimizira definiraju se vrijednosti koje treba ispitati. Algoritam tada evaluira dani model za svaku kombinaciju hiperparametara i vraća kombinaciju ili model koji ostvaruje najbolje rezultate.

Iako se optimalni hiperparametri mogu nalaziti izvan zadanih skupova i neće biti pronađeni, postupak je brz i daje dovoljno dobre rezultate za praktičnu primjenu. Često je dovoljno da pronađe kombinaciju hiperparametara za koju model ne divergira niti prestaje učiti određen broj iteracija.

\subsection{Svojstva}
\label{sec:svojstva}

\subsubsection{Univerzalna aproksimacija}
Teorem univerzalne aproksimacije tvrdi da unaprijedna neuronska mreža može modelirati proizvoljnu Borel mjerljivu funkciju proizvoljno dobro uz nekoliko uvjeta: mora imati linearni izlaz, barem jedan nelinearni skriveni sloj koji koristi sažimajuću funkciju i "dovoljan" broj skrivenih neurona. Dakle, postoji arhitektura i postoje parametri kojima neuronska mreža može modelirati zadani podatkovni skup. No, teorem ne iskazuje kako doći do tih parametara što optimizaciju neuronske mreže čini netrivijalnom \citep[poglavlje~6.4.1]{goodfellowbook}. 

\subsubsection{Reprezentacija dubinom}
Iako je prema teoremu univerzalne aproksimacije dovoljan jedan nelinearni skriveni sloj za predstavljanje proizvoljne Borel mjerljive funkcije, gornja granica veličine tog sloja je eksponencijalno velika naspram broja ulaza što je netraktabilno. Dodavanjem dubine moguće je iskoristiti pravilnosti u funkciji koja se aproksimira kako bi se smanjio potreban broj neurona. Primjer su funkcije simetrične oko neke osi. Ako skriveni slojevi mreže vrše preklapanje te funkcije preko pravca u njenom hiperprostoru, uzastopnim preklapanjem dobiva se sve jednostavnija funkcija. Preklapanje mogu vršiti po dijelovima linearne aktivacijske funkcije poput ReLU i Maxout. Dakako, ne postoji garancija da stvarna funkcija zadovoljava svojstvo simetrije, no u praksi dublje mreže generaliziraju bolje \citep{alexnet, highwaynet, resnet, densenet}. Postoje i druge interpretacije utjecaja dubine, poput svojstva dekompozicije zadatka na manje cjeline ili interpretacije neuronske mreže kao računalnog programa, no one nadilaze temu ovog rada. \citep[poglavlje~6.4.1]{goodfellowbook}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{*VC dimenzija \citep{vc_dimension}}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{*Kompresija podataka}
\todo{https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Generalizacija}
\label{sec:generalizacija}
Model sa svojstvom generalizacije dobro modelira stvarnu funkciju te ispravno radi i na neviđenim primjerima. Ovo je posebno važno pri klasifikaciji slika gdje za pojedinu klasu (npr. automobil) postoji neprebrojivo mogućih slika s različitim modelom, bojom ili kutom gledanja automobila. No dostupno je svega nekoliko stotina tisuća primjera koji se smatraju reprezentativnim za tu klasu i njima se želi izgraditi klasifikator koji je robustan na većinu perturbacija slike. Kako bi model što bolje naučio stvarnu funkciju potreban je velik broj označenih uzoraka. S obzirom da je označavanje podataka često vrlo skupocjeno, istražuju se metode automatizacije poput polu-nadziranog učenja i aktivnog učenja \citep{active_learn}. Augmentacija podataka raznim metodama stvara više primjera iz jednog uzorka stvarne funkcije čime se proširuje podatkovni skup bez potrebe za označavanjem te podiže generalizacijsku moć modela. No, čak i uz navedene metode model i dalje neće dovoljno dobro generalizirati stvarnu funkciju što potvrđuje postojanje neprijateljskih primjera \citep{intriguing_props}.

\subsection{Problemi}
\subsubsection{Odabir hiperparametara}
Arhitektura, prijenosne funkcije i parametri definiraju neuronsku mrežu te njihov pravilan odabir značajno utječe na performanse neuronske mreže. Nažalost nije ih moguće optimalno odabrati u zatvorenom obliku, već se to svodi na problem pretraživanja kao što je pretraživanje po rešetci \secref{sec:grid_search}. Arhitekture mogu poprimiti vrlo složene oblike kao što je predstavljeno radovima \citet{highwaynet}, \citet{resnet}, \citet{densenet}, \citet{inceptionnet} i \citet{yolo} te se najčešće grade ručno s maksimalnom traktabilnom složenošću mreže. Detaljnije o odabiru aktivacijskih funkcija napisano je u poglavlju \ref{sec:aktivacijske_fje}.

\subsubsection{Nestajući gradijent}
\label{sec:nestajući_grad}
Prolaskom kroz duboku mrežu gradijent se množi matricama težina $\mat{W_i}$. Uzastopno množenje može dovesti do problema ako su svojstvene vrijednosti matrice udaljene od $\pm 1$. Rastavom matrice u umnošku ponovljenom $t$ puta \eqref{eq:vanish_grad} vidljiv je problem. Svojstvene vrijednosti veće od $1$ će rasti, a one manje od $1$ će se smanjivati. \citep[poglavlje~8.2.5]{goodfellowbook}
\begin{equation}
\label{eq:vanish_grad}
\mat{W}^t = (\mat{V} \cdot diag(\mat{\lambda}) \cdot \mat{V}^{-1})^t = \mat{V} \cdot diag(\mat{\lambda})^t \cdot \mat{V}^{-1}
\end{equation}

Problem eksplodirajućeg gradijenta rješava se odrezivanjem gradijenta koji je prevelik. Problemu nestajućeg gradijenta moguće je pristupiti uvođenjem preskočnih veza u arhitekturu \citep{highwaynet,resnet,densenet} te odabirom aktivacijske funkcije s linearnim svojstvima \citep{elish}.

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{*Neprijateljski primjeri}
\todo{Problem pretreniranosti + neprijateljski primjeri}

Vrlo složeni modeli, kao što su neuronske mreže, unatoč regularizaciji skloni su pretreniranju. \citep{intriguing_props}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Aktivacijske funkcije}
\label{sec:aktivacijske_fje}
Aktivacijska funkcija unosi nelinearnost u neuronsku mrežu i ima utjecaj na njeno učenje. Prilikom inferencije aktivacijska funkcija stvara nelinearnosti u decizijskoj granici koje su parametrizirane parametrima neurona. U slojevitim arhitekturama aktivacijske funkcije se primjenjuju uzastopno i time vrše nelinearnu projekciju prostora iz jedne dimenzije u drugu te utječu na jednostavnost i kvalitetu konačne klasifikacije. Prilikom učenja neuronske mreže važan je utjecaj derivacije aktivacijske funkcije na gradijent pri širenju unatrag. Prolaskom unatrag gradijent se množi s derivacijom aktivacijske funkcije \eqref{eq:backprop_activation} što može izazvati nestajanje ili eksploziju gradijenta.

Za odabir aktivacijske funkcije ne postoji jasno pravilo. U praksi se odabiru po empirijskim rezultatima i brzinom izvođenja. Iako neke funkcije imaju teorijski potkrijepljena svojstva to ih u praksi ne čini najboljima. Tipično se odabire $ReLU$, a za skrivene slojeve rekurzivnih modela odabire se $tanh$.

Funkcije poput ReLU, njegovih izvedenica i ELiSH mogu sadržavati diskontinuitete u derivaciji funkcije. Pri širenju gradijenta unatrag ti se diskontinuiteti zanemaruju i uzima se vrijednost u koju funkcija teži s jedne strane. Dikontinuiteti u funkciji mogu se interpretirati kao različiti režimi rada neurona. Dobar primjer je ReLU kod kojeg je za negativne ulaze neuron neaktivan.

%\todo{dokaz da su monotone fje dobre}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Aktivacijske funkcije s učećim parametrima}
\todo{članak: učeći parametri u fjama (onaj stari)}

U radu \citet{trained_func} autori istražuju učeće aktivacijske funkcije ...

U radu TAF...

\todo{Spomeni i složenije metode: \citep{network_in_network}}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Periodične aktivacijske funkcije}
U rezultatima pretrage ovog rada i u radu \citet{swish} često pojavljuju se periodične aktivacijske funkcije. Stoga su u nastavku opisani radovi koje je autor uspio pronaći na temu svojstava periodičkih funkcija u neuronskim mrežama.

%\todo{članak: fourier mreža s periodičkim fjama}

Rad \citet{taming_waves} bavi se analizom sinusoide kao aktivacijske funkcije. Periodične aktivacijske funkcije stvaraju brojne lokalne optimume u funkciji gubitka koji otežavaju učenje neuronskih mreža. S obzirom da se mreže treniraju stohastičkim gradijentnim spustom većina lokalnih optimuma nije vidljiva i ublažava naglašenost lokalnih optimuma. Pri uporabi sinusoide autori pokazuju da će model konvergirati samo ako su težine inicijalizirane dovoljno malim vrijednostima da ulaz ne prelazi interval monotonosti sinusa $[-\frac{\pi}{2}, \frac{\pi}{2}]$. U usporedbi sinusoide s ograničenom sinusoidom na potpuno povezanom modelu pokazuje se nezamjetna razliku u performansama iako oko $40\%$ ulaza neurona završava izvan intervala monotonosti, što upućuje da modeli ne ovise snažno o periodičnosti funkcije. U primjeni na rekurzivnim modelima ostvaruje prednost nad monotonim tangensom hiperbolnim, no autori navode teškoću optimizacije u nekim situacijama.

\section{Popularne aktivacijske funkcije}
U nastavku su navedene popularne aktivacijske funkcije koje je autor pronašao u literaturi i koje su isprobane u ovom radu. Funkcije su pasivne u smislu da ne sadrže učeće parametre te njihov izlaz ovisi isključivo o ulazu  u funkciju. Za svaku funkciju napisana je formula i iscrtan izgled funkcije i njene derivacije te navedena neka poznata svojstva.

\subsection{Funkcija identiteta}
\engl{Identity function}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Identity.pdf}
\centering
\caption{Funkcija identiteta i njena derivacija}
\label{fig:identity}
\end{figure}

\begin{equation}
\begin{split}
f(x) = x
\end{split}
\qquad
\begin{split}
f'(x) = 1
\end{split}
\end{equation}

Funkcija identiteta je jednostavna i brza, no njome neuronska mreža može naučiti samo linearne funkcije. Ako se primijeni na dva uzastopna sloja neuronske mreže, konačna će funkcija ponovno biti linearna što znači da se mreža ne može koristiti na nelinearnim podatcima. Derivacija funkcije je konstanta $1$ što znači da je gradijent prolaskom kroz nju očuvan. Funkcija identiteta često se koristi kao funkcija aktivacijske izlaznog sloja kod regresije, no u dubokim arhitekturama \citep{resnet,densenet} i novijim aktivacijskim funkcijama \citep{swish,elish} koristi se paralelno s nelinearnim funkcijama.
\begin{align}
\begin{split}
f_l(x) &= w_l \cdot x + b_l \\
f_1(f_2(x)) &= w_1 \cdot (w_2 \cdot x + b_2) + b_1 \\
&= \underline{w_1 \cdot w_2} \cdot x + \underline{w_1 \cdot b_2 + b_1} \\
&= w_{1,2} \cdot x + b_{1,2}
\end{split}
\end{align}

\subsection{Zglobnica ili ispravljena linearna jedinica (ReLU)}
\label{func:relu}
\engl{Rectified linear unit}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_ReLU.pdf}
\centering
\caption{Funkcija ReLU i njena derivacija}
\label{fig:relu}
\end{figure}

\begin{equation}
\label{eq:relu}
\begin{split}
f(x) &= \begin{cases}
x,		& \text{ako } x > 0 \\
0,		& \otherwise
\end{cases} \\
&= max(0, x)
\end{split}
\qquad
\begin{split}
f'(x) = 
\begin{cases}
1,		& \text{ako } x > 0 \\
0,		& \otherwise
\end{cases}
\end{split}
\end{equation}

Funkcija ReLU svoje korijene povlači iz rada bioloških neurona, a u dubokom učenju je zamijenila dotadašnju sigmoidu i tangens hiperbolni. Zahvaljujući prizemljenosti na negativnoj domeni ReLU omogućava neuronskoj mreži učenje rijetke reprezentacije. Njihovim kombiniranjem u dubokoj mreži dobiva se model s eksponencijalno puno linearnih regija koje dijele zajedničke parametre \citep{relu_rbm}. Zahvaljujući linearnosti na pozitivnoj domeni funkcija ne doprinosi nestajanju ni eksploziji gradijenta, a sam izračun funkcije i derivacije je vrlo brz i efikasan. \citep{relu}

Problem u negativnoj domeni je mogućnost blokiranja gradijenta i neaktivnih neurona (mrtvi neuroni), no dok god postoji put kroz mrežu gdje su neuroni aktivni (u pozitivnoj domeni) učenje će raditi, što pokazuju i rezultati \citep{relu}. Drugi problem je u neograničenosti funkcije u pozitivnoj domeni, no on se rješava regularizacijom koja ograničava veličinu ulaza u neuron \citep{relu}.

\subsection{Propusna ispravljena linearna jedinica (LReLU)}
\engl{Leaky ReLU}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_LReLU.pdf}
\centering
\caption{Funkcija LReLU i njena derivacija za $\alpha=0.1$}
\label{fig:lrelu}
\end{figure}

\begin{equation}
\label{eq:lrelu}
\begin{split}
f(x) &= \begin{cases}
x,			& \text{ako } x > 0 \\
\alpha x,	& \otherwise
\end{cases} \\
&= max(\alpha x, x)
\end{split}
\qquad
\begin{split}
f'(x) = 
\begin{cases}
1,		& \text{ako } x > 0 \\
\alpha,	& \otherwise
\end{cases}
\end{split}
\end{equation}

Problem funkcije ReLU \secref{func:relu} je što postoji mogućnost pojave mrtvih neurona, koji su uvijek u neaktivnom području i kroz njih ne teče gradijent. Taj problem efektivno smanjuje kapacitet modela pod cijenu nepotrebnog memorijskog opterećenja i smanjene brzine izvođenja. Iz navedenih razloga uvedena je propusna ReLU funkcija koja u "neaktivnom" području propušta mali gradijent. Rezultati na zadatku prepoznavanja govora pokazuju da je propusna ReLU funkcija ekvivalentna standardnom ReLU, no rezultati su prikazani na relativno plitkim arhitekturama (do 4 skrivena sloja). Tipična vrijednost parametra $\alpha$ je $0.01$. \citep{lrelu}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Nasumična ispravljena linearna jedinica (RReLU)}
\engl{Randomized leaky ReLU}

\todoimg{}

\begin{equation}
\begin{split}
f(x) &= 
\begin{cases}
x,			& \text{ako } x > 0 \\
\alpha x,	& \otherwise
\end{cases}
\end{split}
\qquad
\begin{split}
f'(x) = 
\begin{cases}
1,		& \text{ako } x > 0 \\
\alpha,	& \otherwise
\end{cases}
\end{split}
\end{equation}
\begin{equation}
\alpha \sim U(l,u),\quad l,u \in [0,1]
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ispravljena linearna jedinica s pragom (ThReLU)}
\engl{Thresholded ReLU}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_ThReLU.pdf}
\centering
\caption{Funkcija ThReLU i njena derivacija}
\label{fig:threlu}
\end{figure}

\begin{equation}
\label{eq:threlu}
\begin{split}
f(x) = 
\begin{cases}
x,		& \text{ako } x > \theta \\
0,		& \otherwise
\end{cases}
\end{split}
\qquad
\begin{split}
f'(x) = 
\begin{cases}
1,		& \text{ako } x > \theta \\
0,		& \otherwise
\end{cases}
\end{split}
\end{equation}

Funkcija je generalizacija funkcije $ReLU$ \secref{func:relu} kojoj je vrijednost praga $0$. Predložena je kao rješenje za pojavu negativnih pragova pri učenju autoenkodera s metodama regularizacije kao što su sažimajući autoenkoder \engl{contractive autoencoder} i autoenkoder za otklanjanje šuma \engl{denoising autoencoder}. Primjenom funkcije bez regularizacije postižu se usporedivi ili bolji rezultati. Za vrijednost praga tipično se uzima $\theta = 1$. \citep{threlu}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{(CReLU)}
\engl{Concatenated ReLU}

\todoimg{}

\begin{equation}
???
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Softplus}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Softplus.pdf}
\centering
\caption{Softplus i njegova derivacija}
\label{fig:softplus}
\end{figure}

\begin{equation}
\label{eq:softplus}
\begin{split}
f(x) = log(1+e^x)
\end{split}
\qquad
\begin{split}
f'(x) = \frac{e^x}{1+e^x}
\end{split}
\end{equation}

Funkcija $Softplus$ je strogo pozitivna monotono rastuća glatka funkcija nalik na $ReLU$. Vrlo je bliska beskonačnom zbroju binomnih jedinica odmaknutih za $1$ koje se mogu koristiti u ograničenim Boltzmann-ovim strojevima. \citep{relu_rbm}
\begin{equation}
\sum_{i=1}^{\infty}\sigma(x-i+0.5) \approx log(1+e^x)
\end{equation}

U odnosu na $ReLU$, funkcija nema problem nestajućeg gradijenta jer propušta gradijent u negativnoj domeni i ostvaruje veće vrijednosti od $ReLU$. To je svojstvo posebna prednost nad $ReLU$ i sigmoidom pri korištenju tehnike Dropout kojom se blokira doprinos velikog broja neurona. Unatoč teoretskim prednostima funkcija rijetko postiže zadovoljive rezultate. \citep{softplus}

%\todoimg{usporedba softplus i relu, možda zgodno za pokazati}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Noisy softplus}

\todoimg{}

\begin{equation}
???
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Eksponencijalno-linearna jedinica (ELU)}
\label{func:elu}
\engl{Exponential linear unit}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_ELU.pdf}
\centering
\caption{Funkcija ELU i njena derivacija}
\label{fig:elu}
\end{figure}

\begin{equation}
\label{eq:elu}
\begin{split}
f(x) = 
\begin{cases}
x,					& \text{ako } x > 0 \\
\alpha (e^x - 1),	& \otherwise
\end{cases}
\end{split}
\qquad
\begin{split}
f'(x) = 
\begin{cases}
1,	 		& \text{ako } x > 0 \\
\alpha e^x,	& \otherwise
\end{cases}
\end{split}
\end{equation}

Funkcija $ELU$ je linearna na pozitivnoj domeni, a u negativnoj domeni postepeno prelazi u zasićenje. Uvedena je kako bi umanjila efekt otklona izlaza neurona, koja nastaje kada je srednja vrijednost izlaza otklonjena od $0$. Aktivacije koje nisu centrirane usporavaju učenje mreže i stoga ih je potrebno centrirati. Jedan pristup je centriranje normalizacijom grupa, a alternativni je odabirom aktivacijske funkcije. $ReLU$ nije centriran jer je u pozitivnoj domeni linearan, a u negativnoj $0$. Tangens hiperbolni je primjer centrirane funkcije. $ELU$ je djelom linearan pa nema problem nestajućeg gradijenta te brže prelazi u zasićenje i time smanjuje razlike između neaktivnih neurona za različite ulazne argumente što ga čini otpornim na šum. Hiperparametar $\alpha$ određuje negativnu vrijednost zasićenja i tipično se postavlja na $1$. Funkcija postiže bolje rezultate od $ReLU$ s normalizacijom grupe. Kada se koristi s normalizacijom grupe ostvaruje nešto lošije rezultate, no i dalje bolje od $ReLU$ i izvedenica. \citep{elu}

\subsection{Skalirana eksponencijalno-linearna jedinica (SELU)}
\engl{Scaled exponential linear unit}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_SELU.pdf}
\centering
\caption{Funkcija SELU i njena derivacija}
\label{fig:selu}
\end{figure}

\begin{equation}
\label{eq:selu}
\begin{split}
f(x) = \lambda
\begin{cases}
x,					& \text{ako } x > 0 \\
\alpha (e^x - 1),	& \otherwise
\end{cases}
\end{split}
\qquad
\begin{split}
f'(x) = \lambda
\begin{cases}
1,	 		& \text{ako } x > 0 \\
\alpha e^x,	& \otherwise
\end{cases}
\end{split}
\end{equation}

Aktivacijska funkcija $SELU$ izgrađena je za ostvarivanje samo-normalizirajuće neuronske mreže kojom je moguće izgraditi duboke potpuno povezane modele bez potrebe za eksplicitnim tehnikama normalizacije. Funkcija nasljeđuje svojstvo otpornosti na nestajuće i eksplodirajuće gradijente od funkcije $ELU$ \secref{func:elu}. Primjenom funkcije na mreži s prilagođenom inicijalizacijom i regularizacijom ostvaruju se rezultati bolji od ekvivalentnih mreža s primijenjenim tehnikama normalizacije (normalizacija težina, slojeva ili grupom) te specijaliziranih mreža (HighwayNet \citep{highwaynet}, ResNet \citep{resnet}) na brojnim podatkovnim skupovima. Performanse samo-normalizirajućih mreža potkrijepljene su teorijom te su definirane vrijednosti parametara $\alpha \approx 1.6733$ i $\lambda \approx 1.0507$ za normalizirane ulaze u sloj. \citep{selu}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{(GELU)}
\engl{Gaussian error linear unit}

\todoimg{}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Swish}
\label{func:swish}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Swish.pdf}
\centering
\caption{Funkcija Swish i njena derivacija}
\label{fig:swish}
\end{figure}

\begin{equation}
\label{eq:swish}
\begin{split}
f(x) = x \cdot \sigma(\beta x)
\end{split}
\qquad
\begin{split}
f'(x) = \sigma(\beta x) \cdot (1 + \beta x \cdot (1-\sigma(\beta x))
\end{split}
\end{equation}
\begin{equation*}
\beta \text{ može biti definiran ili je učeći parametar}
\end{equation*}

Funkcija Swish je pronađena pretragom temeljenom na potpornom učenju. Funkcija zadovoljava strukturu kompozicije binarne (umnožak) i dviju unarnih operacija (identitet i sigmoida). Ispitivanje funkcije na nekoliko podatkovnih skupova i na nekoliko dubokih arhitektura s preskočnim vezama ostvaruje konzistentno bolje rezultate u odnosu na nekoliko dotadašnjih najboljih funkcija bez promjene izvornih hiperparametara. Udubina funkcije na negativnoj domeni pokazuje se važnom jer se većina izlaza nalazi upravo tamo. Parametar $\beta$ interpolira funkciju između linearne funkcije ($\beta=0$) i funkcije $ReLU$ ($\beta \rightarrow \infty$). Parametar $\beta$ može biti učeći parametar koji pospješuje učenje nekih arhitektura, no tipično se postavlja na fiksnu vrijednost $1$. \citep{swish}

\subsection{ELiSH}
\label{func:elish}
\engl{Exponential Linear Sigmoid Squashing}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_ELiSH.pdf}
\centering
\caption{Funkcija ELiSH i njena derivacija}
\label{fig:elish}
\end{figure}

\begin{equation}
\begin{split}
&f(x) = 
	\begin{cases}
		swish(x), \quad x \geq 0 \\
		ELU(x) \cdot \sigma(x), \quad \otherwise
	\end{cases} \\
&f'(x) =
	\begin{cases}
		swish'(x), \quad x \geq 0 \\
		\sigma(x) \cdot (ELU'(x) + ELU(x) \cdot (1-\sigma(x)), \quad \otherwise
	\end{cases}
\end{split}
\end{equation}

Ova funkcija i njena brža aproksimacija \textit{Tvrdi ELiSH} su kompozicije funkcija, različito definirane na pozitivnoj i negativnoj domeni. ELiSH pozitivnu domenu nasljeđuje od funkcije Swish \eqref{eq:swish}, a negativna je umnožak funkcije ELU \eqref{eq:elu} i sigmoide \eqref{eq:sigmoid}. Obje funkcije su ručno izgrađene s ciljem iskorištavanja dobrog prijenosa informacije koji ostvaruju funkcije Swish i sigmoida te sprečavanjem nestajućeg gradijenta pomoću linearne komponente u funkciji Swish. \citep{elish}

Funkcija izgledom podsjeća na funkciju Swish, no sadrži diskontinuitet u ishodištu, po uzoru na ReLU.

\subsection{Tvrdi ELiSH}
\label{func:hard_elish}
\engl{Hard ELiSH}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Hard_ELiSH.pdf}
\centering
\caption{Funkcija Tvrdi ELiSH i njena derivacija}
\label{fig:hard_elish}
\end{figure}

\begin{equation}
\begin{split}
&f(x) =
	\begin{cases}
		x \cdot a(x), \quad x \geq 0 \\
		ELU(x) \cdot a(x), \quad \otherwise
	\end{cases} \\
&f'(x) = \begin{cases}
	a(x) + x \cdot a'(x), \quad x \geq 0 \\
	ELU'(x) \cdot a(x) + ELU(x) \cdot a'(x), \quad \otherwise
\end{cases}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
&a(x) = \max(0, \min(1, \frac{x+1}{2})) \approx HardSigmoid(x) \\
&a'(x) =
\begin{cases}
	0.5, \quad |x| \leq 1 \\
	0, \quad \otherwise
\end{cases}	
\end{split}
\end{equation}

Ova funkcija je aproksimacija funkcije ELiSH opisane u poglavlju \ref{func:elish}. Uvedena je za potrebe bržeg izvođenja sigmoide i nasljeđuje svojstva ELiSH funkcije. \citep{elish}

\subsection{Ograničena ispravljena linearna jedinica (ReLU-n)}
\label{func:relun}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_ReLU6.pdf}
\centering
\caption{Funkcija ReLU6 i njena derivacija}
\label{fig:relu6}
\end{figure}

\begin{equation}
\label{eq:relun}
\begin{split}
f(x) &= \begin{cases}
0,		& \text{ako } x < 0 \\
x,		& \text{ako } x \in [0, n] \\
n,		& \otherwise
\end{cases} \\
&= min(n, max(0, x))
\end{split}
\qquad
\begin{split}
f'(x) = 
\begin{cases}
1,		& \text{ako } x \in [0, n] \\
0,		& \otherwise
\end{cases}
\end{split}
\end{equation}
\begin{equation*}
n \in \realnum
\end{equation*}

Ograničena ReLU funkcija uvedena je kako bi ograničeni Boltzmann-ov stroj \engl{Restricted Boltzmann machine} brže naučio rijetke značajke, koje kasnije ugađa konvolucijski klasifikator. Za razliku od ReLU koji se može interpretirati kao kompozicija beskonačno mnogo identičnih Bernoullijevih jedinica translatiranih po domeni, ograničeni ReLU interpretira se kao kompozicija $n$ Bernoullijevih jedinica. Za vidljivi sloj autori koriste ReLU1, a za skrivene slojeve ReLU6. \citep{relu6}

S obzirom da je ReLUn neuron aktivan samo na relativno uskoj regiji u pozitivnoj domeni, postoji opasnost od pojave mrtvih neurona (kroz koje ne prolazi gradijent). Idealna regija je $<0,n>$ jer  je u njoj neuron aktivan, a učenjem se može specijalizirati približavanjem zasićenju i stvoriti rijetke reprezentacije. Stoga je potrebno koristiti regularizaciju te pažljivu inicijalizaciju parametara.

\subsection{Sigmoida ($\sigma $)}
\engl{Sigmoid}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Sigmoid.pdf}
\centering
\caption{Sigmoida i njena derivacija}
\label{fig:sigmoid}
\end{figure}

\begin{equation}
\label{eq:sigmoid}
\begin{split}
f(x) = \frac{1}{1+e^{-x}}
\end{split}
\qquad
\begin{split}
f'(x) = \sigma(x)(1-\sigma(x))
\end{split}
\end{equation}

Sigmoida kao aktivacijska funkcija svoje korijene vuče iz biologije \citep[str.~16]{neuroscience}. Funkcija je kontinuirana i strogo pozitivna sa zasićenjem u $1$ na pozitivnoj domeni i zasićenjem u $0$ na negativnoj domeni. Funkciju se može smatrati relaksiranom funkcijom skoka. Za velike parametre neurona sigmoida se približava funkciji skoka te za malu perturbaciju ulaza oko 0 daje veliku promjenu izlaza što otežava postupak učenja. Derivacija sigmoide je zvonolika funkcija s vrhom u $0.25$. Množenjem gradijenta s njenom derivacijom \eqref{eq:loss_classification_softmax_deriv} gradijent se brzo smanjuje što pridonosi pojavi nestajućeg gradijenta. Iz tog razloga je gotovo nemoguće učiti duboke modele sa sigmoidom te su razvijene nove funkcije s ciljem očuvanja gradijenta.

\subsection{Tvrda sigmoida}
\label{func:hard_sigmoid}
\engl{Hard sigmoid}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Hard_sigmoid.pdf}
\centering
\caption{Tvrda sigmoida i njena derivacija}
\label{fig:hard_sigmoid}
\end{figure}

\begin{equation}
\label{eq:hsigmoid}
\begin{split}
f(x) = min(1,\ max(0,\ 0.2x + 0.5))
\end{split}
\qquad
\begin{split}
f'(x) = 
\begin{cases}
0.2,	 		& \text{ako } |x| \leq 2.5 \\
0,	& \text{inače}
\end{cases}
\end{split}
\end{equation}

Funkcija je tvrda inačica sigmoide nastala Taylor-ovim razvojem prvog reda oko nule. Funkcija omogućava učenje oštrih granica, pod cijenu zaustavljanja gradijenta u regijama zasićenja. U regije zasićenja funkcije moguće je dodati šum koji sprječava zaustavljanje gradijenta i pomaže optimizaciji. Kako učenje napreduje, šum se postepeno smanjuje kako bi se osigurala konvergencija algoritma. \citep{hardsigm}

Iako se u ovom radu pri primjeni tvrde funkcije ne dodaje šum, funkcija ostvaruje bolje rezultate od popularne funkcije $ReLU$.

\subsection{Tangens hiperbolni (tanh)}
\label{func:tanh}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_tanh.pdf}
\centering
\caption{Funkcija tanh i njena derivacija}
\label{fig:tanh}
\end{figure}

\begin{equation}
\label{eq:tanh}
\begin{split}
f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{split}
\qquad
\begin{split}
f'(x) = 1 - tanh^2(x)
\end{split}
\end{equation}

Tangens hiperbolni je sličan sigmoidi, no nije strogo pozitivan. Zasićenja u $\pm 1$ čine ga pogodnim za rekurzivne modele koji zahtijevaju sposobnost dodavanja i oduzimanja vrijednosti između slojeva. Derivacija je oštrija i poprima veće vrijednosti u odnosu na sigmoidu, s vrhom u $1$, što je pogodnije za prijenos gradijenta pri učenju dubokih i rekurzivnih modela.

\subsection{Tvrdi tangens hiperbolni}
\engl{Hard tanh}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Hard_tanh.pdf}
\centering
\caption{Tvrdi tanh i njegova derivacija}
\label{fig:hard_tanh}
\end{figure}

\begin{equation}
\label{eq:htanh}
\begin{split}
f(x) =
\begin{cases}
-1,	 		& \text{ako } x < 1 \\
x,	 		& \text{ako } |x| \leq 1 \\
1,	& \text{inače}
\end{cases}
\end{split}
\qquad
\begin{split}
f'(x) =
\begin{cases}
0,	 		& \text{ako } x < 1 \\
1,	 		& \text{ako } |x| \leq 1 \\
0,	& \text{inače}
\end{cases}
\end{split}
\end{equation}

Tvrdi tangens hiperbolni je linearizirana aproksimacija tangensa hiperbolnog korištena u radu \citet{collobert_phd}. Iako nema glatkoću izvorne funkcije i ne propušta gradijent izvan intervala $[-1,1]$, jednako dobro pomaže generalizaciji mreže. Koristan je zbog veće brzine izvođenja funkcije i derivacije u odnosu na tangens hiperbolni.

U radu \citet{hardsigm} u regije zasićenja dodaje se šum kao što je opisano u poglavlju \ref{func:hard_sigmoid}. Iako se u ovom radu ne dodaje šum kao ni kod tvrde sigmoide, funkcija ostvaruje rezultate bolje od popularne funkcije $ReLU$.

\subsection{Racionalna aproksimacija tanh}
\engl{Rational tanh}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Rational_tanh.pdf}
\centering
\caption{Racionalni tanh i njegova derivacija}
\label{fig:rational_tanh}
\end{figure}

\begin{align}
\label{eq:rattanh}
\begin{split}
f(x) &= 1.7159 \cdot tanh^*(\frac{2}{3}x), \quad
tanh^*(x) = sgn(x)(1 - \frac{1}{1 + |x| + x^2 + 1.41645 \cdot x^4}) \\
f'(x) &= 1.7159 \cdot \frac{2}{3} \cdot tanh^{*'}(\frac{2}{3}x), \quad
tanh^{*'}(x) = \frac{1+sgn(x) \cdot (2x + 4 \cdot 1.41645 \cdot x^3)}{(1 + |x| + x^2 + 1.41645 \cdot x^4)^2}
\end{split}
\end{align}

Racionalna aproksimacija tangensa hiperbolnog prati izvornu funkciju s relativnom greškom manjom od $1.8\%$, no uvelike ubrzava njegovo računanje. Umjesto zahtjevnog računanja eksponenta prirodne konstante ova aproksimacija zahtjeva samo 11 naredbi (uzastopno kvadriranje ulaza). Funkcija se pokazala boljom od $ReLU$ na problemu detekcije lica u slikama. Odabir konstanti kojima su skalirani ulaz i izlaz funkcije nije opisan. \citep{rattanh}

\subsection{Ispravljeni tanh}
\engl{Rectified tanh}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Rectified_tanh.pdf}
\centering
\caption{Ispravljeni tanh i njegova derivacija}
\label{fig:rectified_tanh}
\end{figure}

\begin{equation}
\label{eq:rectanh}
\begin{split}
f(x) &=
\begin{cases}
tanh(x),	 		& \text{ako } x > 0 \\
0,	& \text{inače}
\end{cases} \\
&= ReLU(tanh(x))
\end{split}
\qquad
\begin{split}
f'(x) =
\begin{cases}
1-tanh^2(x),	 		& \text{ako } x > 0 \\
0,	& \text{inače}
\end{cases}
\end{split}
\end{equation}

Ispravljeni tangens hiperbolni je strogo pozitivan, sadrži oštru nelinearnost u $0$ i na pozitivnoj domeni prelazi u zasićenje. Funkcija je uvedena radi ispitivanja utječe li magnituda izlaza na klasifikaciju s Dropout-om ili je dovoljna binarna informacija je li neuron upaljen ili ugašen. Pri inferenciji mreže izlazi prolaze kroz 0-1 funkciju te postaju binarni. Usporedbom funkcije s $ReLU$ na potpuno povezanom modelu pokazuje se da su performanse malo lošije. Interpretacija autora je da modeli ne ostvaruju značajan doprinos od neograničenog linearnog dijela $ReLU$ funkcije. \citep{rectanh}

\subsection{Softsign}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Softsign.pdf}
\centering
\caption{Softsign i njegova derivacija}
\label{fig:softsign}
\end{figure}

\begin{equation}
\label{eq:softsign}
\begin{split}
f(x) = \frac{x}{1+|x|}
\end{split}
\qquad
\begin{split}
f'(x) = \frac{1 + |x| - x \cdot sign(x)}{(1+|x|)^2}
\end{split}
\end{equation}

Funkcija dijeli sličnost s tangensom hiperbolnim \secref{func:tanh}, no za razliku od eksponencijalno konvergirajuće funkcije $tanh$, $softsign$ konvergira kvadratno. Za razliku od $tanh$ čiji slojevi sekvencijalno prelaze u zasićenje, svi $softsign$ slojevi sporije prelaze u zasićenje i to rade zajednički. Aktivacije $tanh$ gomilaju se u ekstremima (u zasićenju) i u sredini, dok kod $softsign$ osim sredine većina aktivacija djeluje na zglobovima funkcije u kojima je nelinearnost najveća, a gradijenti i dalje prolaze. \citep{xavier}

Funkcija je izvorno predstavljena u radu \citet{softsign_orig} i testirana s nekoliko eksperimenata u području računalnog vida. No, taj rad je u vremenu pisanja ovog rada autorima bio nedostupan. Stoga je ostavljena referenca za znatiželjne čitatelje.

\subsection{Sinus (sin)}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Sin.pdf}
\centering
\caption{Sinusoida i njegova derivacija}
\label{fig:sin}
\end{figure}

\begin{equation}
\label{eq:sin}
\begin{split}
f(x) = sin(x)
\end{split}
\qquad
\begin{split}
f'(x) = cos(x)
\end{split}
\end{equation}

%Sinus je periodička funkcija te nema teorijsku podlogu za uporabu u neuronskim mrežama, za razliku od monotonih funkcija.
%\todo{članak dokaz univerzalne aproksimacije monotonih fja}

Sinusoida se često pojavljuje u radovima pretrage aktivacijskih funkcija \citep{elish} pa tako i u ovom radu te pokazuje obećavajuće rezultate. U radu \citet{taming_waves} autori predstavljaju problematiku učenja sa sinusoidom na jednostavnom zadatku aproksimacije sinusoide. Problem stvaraju brojni lokalni optimumi na koje je izrazito osjetljiv gradijentni spust. Problem ublažava učenje stohastičkim gradijentnim spustom koji zaglađuje valovitost funkcije gubitka. Dodatno, autori pokazuju da neuronska mreža zapravo ne ovisi snažno o periodičnosti funkcije tako da su rezultate usporedili s ograničenom sinusoidom \eqref{eq:trsin}.

Po uzoru na slične funkcije kao što je sigmoida \figref{fig:sigmoid} i tangens hiperbolni \figref{fig:tanh}, sinusoida najveću vrijednost gradijenta daje upravo kada je aktivacija jednaka 0 (u ograničenom intervalu).

\subsection{Ograničeni sinus (TrSin)}
\label{func:trsin}
\engl{Truncated sine}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_TrSin.pdf}
\centering
\caption{Ograničeni sinus i njegova derivacija}
\label{fig:trsin}
\end{figure}

\begin{equation}
\label{eq:trsin}
\begin{split}
f(x) =
\begin{cases}
0, \quad x < \frac{\pi}{2} \\
sin(x), \quad |x| \leq \frac{\pi}{2} \\
1, \quad \otherwise
\end{cases}
\end{split}
\qquad
\begin{split}
f'(x) =
\begin{cases}
cos(x), \quad |x| \leq \frac{\pi}{2} \\
0, \quad \otherwise
\end{cases}
\end{split}
\end{equation}

Ograničeni sinus korišten je u radu \citet{taming_waves} za usporedbu s klasičnom sinusoidom i tangensom hiperbolnim. U usporedbi sa sinusom ispituje se utjecaj periodičnosti sinusa na performanse. S tangensom hiperbolnim uspoređene su performanse zbog sličnosti u obliku krivulja.

\subsection{Kosinus (cos)}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Cos.pdf}
\centering
\caption{Kosinus i njegova derivacija}
\label{fig:cos}
\end{figure}

\begin{equation}
\label{eq:cos}
\begin{split}
f(x) = cos(x)
\end{split}
\qquad
\begin{split}
f'(x) = -sin(x)
\end{split}
\end{equation}

Kosinus je funkcija sinusa pomaknuta za četvrtinu periode te dijeli ista svojstva i probleme. no, s obzirom da je pomak relativno velik u odnosu na očekivane veličine ulaza, može se promatrati kao zasebna aktivacijska funkcija. Derivacija kosinusa se poprilično razlikuje od ostalih aktivacijskih funkcija. Unatoč tome, daje vrlo kompetitivne rezultate \secref{sec:rezultati}.
\begin{equation}
cos(x) = sin(x + \frac{\pi}{2}) \approx sin(x + 1.571)
\end{equation}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ograničeni kosinus (TrCos)}
\engl{Truncated cosine}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_TrCos.pdf}
\centering
\caption{Ograničeni kosinus i njegova derivacija}
\end{figure}

\begin{equation}
\begin{split}
f(x) =
\begin{cases}
cos(x), \quad |x| \leq \frac{\pi}{2} \\
0, \quad \otherwise
\end{cases}
\end{split}
\qquad
\begin{split}
f'(x) =
\begin{cases}
-sin(x), \quad |x| \leq \frac{\pi}{2} \\
0, \quad \otherwise
\end{cases}
\end{split}
\end{equation}

Ograničeni kosinus autori nisu pronašli u literaturi, no navodi se zbog potrebe ispitivanja ovisnosti o periodičnosti kosinusa (po uzoru na poglavlje \ref{func:trsin}).
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Parabola $x^2$}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Pow2.pdf}
\centering
\caption{Parabola i njena derivacija}
\end{figure}

\begin{equation}
\begin{split}
f(x) = x^2
\end{split}
\qquad
\begin{split}
f'(x) = 2x
\end{split}
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Kubna parabola $x^3$}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Pow3.pdf}
\centering
\caption{Kubna parabola i njena derivacija}
\end{figure}

\begin{equation}
\begin{split}
f(x) = x^3
\end{split}
\qquad
\begin{split}
f'(x) = 3x^2
\end{split}
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gaussova krivulja bez normalizacije}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Gauss.pdf}
\centering
\caption{Gaussova krivulja bez normalizacije i njena derivacija}
\label{fig:gauss}
\end{figure}

\begin{equation}
\label{eq:gauss}
\begin{split}
f(x) = e^{-x^2}
\end{split}
\qquad
\begin{split}
f'(x) = -2x \cdot f(x)
\end{split}
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Softmax}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_Softmax.pdf}
\centering
\caption{Tri različita ulazna pravca te odziv funkcije $Softmax$ i njene derivacije na pravce}
\label{fig:softmax}
\end{figure}

\begin{equation}
\label{eq:softmax}
\begin{split}
f(\vec{x}) = \frac{e^{\vec{x}}}{\sum_ie^{\vec{x}_i}}
\end{split}
\qquad
\begin{split}
f'(\vec{x}) = f(\vec{x}) \cdot (1-f(\vec{x}))
\end{split}
\end{equation}

Softmax se tipično koristi kao izlazni sloj klasifikacijske mreže. Funkcija nije skalarna već na ulazu zahtjeva vektor. Eksponentom ulaza ističu se veće vrijednosti ulaza, a dijeljenjem sa sumom normaliziraju izlazi (suma izlaza je jednaka $1$). Time se izlaz mreže može smatrati vjerojatnosnom distribucijom što je izrazito korisno za zadatke klasifikacije. Osim što se iz izlaza može zaključiti koja je klasa najvjerojatnija, funkcija definira i koliko je mreža sigurna u svoju predikciju. Njenim korištenjem drastično se pojednostavljuje derivacija funkcije gubitka za klasifikaciju \eqref{eq:loss_classification_softmax} što ubrzava učenje mreže.
% Korištenjem funkcije u skrivenim slojevima mreže ostvaruje se kaskada klasifikatora, jer time svaki sloj predstavlja klasifikator na temelju značajki prethodnog sloja.

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Maxout}

\todoimg{}

\begin{equation}
???
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adaptivne aktivacijske funkcije}
U nastavku su navedene adaptivne aktivacijske funkcije koje je autor pronašao u literaturi. Funkcije su adaptivne jer sadrže parametre koje je moguće optimizirati. Njihov izlaz ovisi o ulazu u funkciju i adaptivnim parametrima. Za svaku funkciju napisana je formula i iscrtan izgled funkcije i njene derivacije te navedena neka poznata svojstva.

\subsection{Parametrizirana ispravljena linearna jedinica (PReLU)}
\engl{Parametric ReLU}

\todoimg{}

\begin{equation}
\begin{split}
f(x) &= \begin{cases}
x,			& \text{ako } x > 0 \\
\alpha x,	& \otherwise
\end{cases} \\
&= max(\alpha x, x)
\end{split}
\qquad
\begin{split}
f'(x) = 
\begin{cases}
1,		& \text{ako } x > 0 \\
\alpha,	& \otherwise
\end{cases}
\end{split}
\end{equation}
\begin{equation*}
\alpha \text{ je učeći parametar}
\end{equation*}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}

\subsection{Adaptivna razlomljena linearna funkcija (APL)}
\engl{Adaptive piecewise linear}

\todoimg{}

\begin{equation}
\begin{split}
f(x) &=  \\
\end{split}
\qquad
\begin{split}
f'(x) = 
\end{split}
\end{equation}
\begin{equation*}
\alpha \text{ je učeći parametar}
\end{equation*}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}

\subsection{ReLU oblika S (S-ReLU)}
\engl{S-shaped ReLU}

\begin{figure}[H]
\includegraphics[width=\textwidth]{func_PLU.pdf}
\centering
\caption{Funkcija PLU i njena derivacija}
\end{figure}

\begin{equation}
\begin{split}
f(x) &=
\begin{cases}
x, \quad |x| \leq c \\
\alpha \cdot x, \quad \otherwise
\end{cases} \\
&= \max(\alpha x - c(1-\alpha), \min(x, \alpha x + c (1-\alpha)))
\end{split}
\quad
\begin{split}
f'(x) =
\begin{cases}
1, \quad |x| \leq c \\
\alpha, \quad \otherwise
\end{cases}
\end{split}
\end{equation}
\begin{equation*}
\alpha, c \in \realnum
\end{equation*}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}

\section{Aktivacijske funkcije posebne namjene}
U nastavku su navedene aktivacijske funkcije posebne namjene koje je autor pronašao u literaturi. Navedene funkcije uvedene su za pospješivanje neuronskih mreža specifičnih arhitektura. Funkcije nisu korištene u ovom radu, no autor smatra da ih je dobro spomenuti. Za svaku funkciju napisana je formula i iscrtan izgled funkcije i njene derivacije te navedena neka poznata svojstva.

\subsection{(DReLU)}
\engl{Dual ReLU}

\todoimg{}

\begin{equation}
???
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}

Koristi se u rekurzivnim neuronskim mrežama kako bi se ... LSTM

\subsection{Hijerarhijski softmax}
\engl{hierarchical softmax}

\todoimg{}

\begin{equation}
???
\end{equation}

\todo{koji problem rješava}
\todo{svojstva}
\todo{problemi}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%--------------------------------------------------------------------------------------%
\chapter{Optimizacija aktivacijske funkcije genetskim programiranjem}

\section{Genetsko programiranje}
Genetsko programiranje je područje evolucijskog računarstva koje se bavi algoritmima pretrage hijerarhijskih struktura. Razlikuje se od susjednih područja po tome što je genotip izvršivi program i dobrota jedinke se dobiva njenim izvršavanjem. Genotip tipično poprima stablastu strukturu te u pravilu nema zadanu veličinu, no može se ograničiti. Dijelovi genotipa definirani su jezikom, skupom funkcija i listova koje definiraju prostor pretraživanja. U ovom radu se elementi jezika nazivaju čvorovima. Jezik mora biti zatvoren (izlaz svakog čvora je kompatibilan s ulazom svakog drugog čvora) i dovoljan (rješenje je moguće predstaviti isključivo koristeći elemente jezika). \citep{comp_intelligence}

Evolucijski algoritmi su intrinzično stohastični te omogućuju globalno i lokalno pretraživanje prostora, za razliku od gradijentnog spusta koji lokalno pretražuje prostor. Dok se lokalnim pretraživanjem pretražuje usko susjedstvo oko točke u prostoru, globalno pretraživanje pretražuje točke po čitavoj domeni unutar dosega. To svojstvo osigurava otpornost evolucijskih algoritama na prepreke koje ograničavaju gradijentni spust \secref{sec:gradijentni_spust}. Opseg pretrage određen je jezikom te ograničenjima pri inicijalizaciji populacije i operatorima pretrage. Treba razlikovati genotipski i fenotipski prostor. Genotipski prostor definiran je jezikom i najvećom dubinom stabla, dok je fenotipski prostor definiran dekoderom koji genotip pretvara u primjenjivo rješenje. Iako ograničenja u genotipu ograničavaju fenotip, mogu imati nepredvidiv utjecaj.

\subsection{Građa}
Kao i ostali evolucijski algoritmi, algoritam genetskog programiranja sastoji se od više elemenata koji definiraju način pretrage. \textbf{Inicijalizator populacije} služi stvaranju početnih točaka iz kojih kreće pretraga i poželjno je da stvara što raznolikija rješenja (globalna pretraga). \textbf{Operator odabira} iz populacije uzima po jedan par jedinki za roditelje i jednu ili dvije jedinke za zamjenu. Pri RouletteWheel odabiru vjerojatnosti odabira jedinki proporcionalna je njihovoj dobroti. Pri turnirskom odabiru nasumično se odabire $n$ jedinki, dvije najbolje postaju roditeljima te ona najlošija biva odabrana za zamjenu. Operatori odabira naginju zadržavanju dobrih genotipa u populaciji i time osiguravaju konvergenciju algoritma (lokalna pretraga). \textbf{Operator križanja} iz genotipa odabranih roditelja izgradi jedno ili dva djeteta koja će biti dio nove populacije. Operatori križanja miješaju genotipove jedinki zadržavajući svojstva dobrih i time pomažu konvergenciji algoritma (lokalna pretraga). \textbf{Operator mutacije} nad stvorenim djetetom unosi nasumičnu izmjenu i time osigurava unos novih informacija u populaciji (globalna pretraga). Često se definira po nekoliko različitih operatora križanja i mutacije koji različito djeluju na pretragu te se time upravlja globalnošću pretrage. Pristup izgradnji nove populacije na temelju stare utječe na snagu konvergencije algoritma. \textbf{Generacijski pristup} će u svakoj iteraciji čitavu populaciju zamijeniti djecom. Pri tome često se koristi \textbf{elitizam} kojim se garantira očuvanje najbolje jedinke iz stare u novu populaciju. \textbf{Eliminacijski pristup} definira postotak populacije koji će biti zamijenjen djecom.

Evolucijski algoritmi podržavaju nekoliko glavnih hiperparametara. \textbf{Veličina populacije} definira broj istovremenih točaka u prostoru te o njoj ovisi vjerojatnost pronalaska optimuma. Operatori mutacije unose izmjene u genomu i njihova primjena se regulira \textbf{vjerojatnošću primjene}. \textbf{Kriteriji zaustavljanja} određuju završetak pretrage i mogu ovisiti o kvaliteti rješenja ili raspoloživim resursima. \textbf{Broj iteracija} analogan je onome u gradijentnom spustu \secref{sec:gradijentni_spust} i često se koristi kao gornja granica za vremenski trošak izvođenja. \textbf{Broj evaluacija} je koristan kada je poznato trajanje pojedine evaluacije želi se sukladno raspoloživom vremenu odrediti veličina populacije. \textbf{Raspoloživo vrijeme} se koristi kao i broj iteracija, ali u situacijama kada nije poznato trajanje pojedine iteracije algoritma. \textbf{Kvaliteta rješenja} je koristan kriterij zaustavljanja ako postoji definirana granica zadovoljavajuće kvalitete. Razne inačice evolucijskih algoritama mogu uvesti brojne dodatne hiperparametre.

\begin{algorithm}[H]
\begin{algorithmic}
\STATE{\textbf{Ulaz:} \\
	$f(x)$ -- funkcija, \\
	$K$ -- kriterij zaustavljanja, \\
	$N$ -- veličina populacije \\
}
$InicijalizirajPopulaciju(populacija, N)$ \\
\WHILE{$K$ nije zadovoljen} \STATE {
	$x^* \gets OdaberiNajbolju(x, populacija)$ \\
	definiraj novu populaciju: $populacija'$ \\
	$DodajPopulaciji(x^*, populacija')$ \\
	\FOR{$N-1$ puta} \STATE {
		$(p_1, p_2) \gets OdaberiRoditelje(populacija)$ \\
		$c \gets OperatorKrižanja(p_1, p_2)$ \\
		$OperatorMutacije(c)$ \\
		$DodajPopulaciji(c, populacija')$ \\
	} \ENDFOR \\
	$populacija \gets populacija'$ \\
} \ENDWHILE \\
$x^* \gets OdaberiNajbolju(x, populacija)$ \\
\STATE{\textbf{Izlaz:} \\
	$x^*$ -- pronađena optimalna jedinka
}
\end{algorithmic}
\caption{Klasičan generacijski evolucijski algoritam}
\label{alg:ea}
\end{algorithm}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Simbolička regresija}
Simbolička regresija je postupak izgradnje funkcije koja dobro aproksimira uzorke stvarne funkcije.
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Neuroevolucija genetskim programiranjem}
Proizvoljnu neuronsku mrežu moguće je predstaviti stablom, gdje su čvorovi mreže ulazne i aktivacijske funkcije neurona te dodatne operacije (normalizacija grupe, dropout i ostali). Takav pristup korišten je u \citet{evo_atari} algoritmom kartezijevog genetskog programiranja. Metoda NEAT dijeli graf na neurone i težine istovremeno pretražuje prostor arhitektura i težina \citep{neat}. Postoje i metode indirektne evolucije arhitekture neuronskih mreža čiji genotip je potrebno dekodirati. Takve metode smanjuju prostor pretraživanja i time efikasno istražuju vrlo složene arhitekture \citep{gruau}.

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo{negdje spomeni da su neuralke zapravo vrlo ograničene jer se ograničava prostor mogućnosti zadavanjem fiksnih aktivacijskih fja, arhitekture i načina optimizacije (induktivna pristranost ograničavanjem skupa hipoteza). leži negdje između GP i ručno izgrađenih modela (jer je neuralka samo stablo funkcija kao u TF). CGP unosi ograničenje strukture što je bliže neuralki i daje zanimljive rezultate (atari cgp). Čini se da im godi balans između strukture i nasumičnosti.}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optimizacija aktivacijskih funkcija}
U literaturi postoje brojni radovi na temu optimizacija aktivacijskih funkcija, neki od kojih se služe genetskim programiranjem. U nastavku su detaljnije objašnjeni radovi temu optimizacije aktivacijskih funkcija koji utječu na ovaj rad.

U radu \citet{swish} autori smanjuju prostor pretraživanja definirajući strukturu funkcija koje se pretražuju u obliku bloka. Blokove definiraju kompozicijom binarne i dviju unarnih funkcija te istražuju rekurzivna proširenja istog bloka (umjesto jedne unarne funkcije ponavlja se blok). Rekurzivna mreža predviđa funkciju na idućem mjestu u bloku temeljem prethodno odabrane funkcije. Izgrađeni blok koristi se kao aktivacijska funkcija dubokih mreža te se postignuta točnost uzima kao vrijednost nagrade za optimizaciju potpornim učenjem. Rezultat pretrage je funkcija $Swish$ \secref{func:swish} koja konzistentno nadjačava $ReLU$ \secref{func:relu} i ostale popularne funkcije.

U radu \citet{elish} autori koriste hibridni genetski algoritam kako bi evoluirali funkciju različito definiranu na pozitivnoj i negativnoj domeni. Dijelovi funkcije su predstavljeni stablima i križaju ih posebnim operatorima koji odvojeno mijenjaju pozitivnu i negativnu stranu. Skup čvorova su osnovne aritmetičke operacije, a listovi su popularne aktivacijske funkcije bez konstanti. Autori predstavljaju i nove aktivacijske funkcije (poglavlja \ref{func:elish}, \ref{func:hard_elish}) koje su ručno izgradili s ciljem kombiniranja dobrih svojstava manjih funkcija. Na tri podatkovna skupa pokazuju da su njihove funkcije najbolje.

Rad \citet{cube_spline_func} predstavlja aktivacijsku funkciju definiranu kubnim splajnom. Koristi se genetski algoritam za pronalazak točaka kojima se interpolira aktivacijska funkcija te pronalazak arhitekture čitave mreže. Rezultati pokazuju da pronađene funkcije ostvaruju poboljšanje nad sigmoidom na zadatcima klasifikacije.

\section{Genetsko programiranje s tabu listom}
Klasičan algoritam genetskog programiranja često dovodi do stvaranja identičnih jedinki u populaciji što loše utječe na globalnost pretrage. Stoga se u ovom radu unosi primjena tabu liste u proces algoritma. Nakon stvaranja djeteta klasičnim koracima (križanje i mutacija) dodaje se dodatan korak u kome se dijete mutira dok ne postane jedinstveno. Kako algoritam ne bi zapeo definira se gornja granica dozvoljenog broja mutacija. Kada je stvoreno jedinstveno dijete, ono se dodaje u populaciju i zabilježi na kraj tabu liste.

Veličina tabu liste definira koliko se često smije pojaviti već postojeća jedinka u populaciji. U ovom radu se istražuje utjecaj na razini broja iteracija. Na primjer, ako je populacija veličine $10$ jedinki tada će veličina tabu liste za $2$ iteracije biti veličine $20$. Za vrijednost $0$ algoritam se ponaša identično klasičnom algoritmu genetskog programiranja.

Zbog ponovljenog mutiranja jedinke treba paziti na balans između mutacija koje povećavaju ili smanjuju dubinu stabla. U suprotnom će jedinke vrlo brzo postati vrlo velike što je nepoželjno za pretraživanje aktivacijskih funkcija \citep{swish} ili će ostati suviše male što ograničava pretraživanje šireg prostora.

\begin{algorithm}[H]
\begin{algorithmic}
\STATE{\textbf{Ulaz:} \\
	$f(x)$ -- funkcija, \\
	$K$ -- kriterij zaustavljanja, \\
	$N$ -- veličina populacije, \\
	$T$ -- veličina tabu liste \\
}
$InicijalizirajPopulaciju(populacija, N)$ \\
$tabu \gets StvoriPraznuTabuListu(T)$ \\
\WHILE{$K$ nije zadovoljen} \STATE {
	$x^* \gets OdaberiNajbolju(x, populacija)$ \\
	definiraj novu populaciju: $populacija'$ \\
	$DodajPopulaciji(x^*, populacija')$ \\
	\FOR{$N-1$ puta} \STATE {
		$(p_1, p_2) \gets OdaberiRoditelje(populacija)$ \\
		$c \gets OperatorKrižanja(p_1, p_2)$ \\
		$OperatorMutacije(c)$ \\
		\WHILE{$x \in tabu$} \STATE {
			$OperatorMutacije(c)$ \\
		} \ENDWHILE \\
		$DodajNaKraj(c, tabu)$ \\
		\IF{$|tabu| > T$} \STATE {
			$UkloniPrvog(tabu)$ \\
		} \ENDIF \\
		$DodajPopulaciji(c, populacija')$ \\
	} \ENDFOR \\
	$populacija \gets populacija'$ \\
} \ENDWHILE \\
$x^* \gets OdaberiNajbolju(x, populacija)$ \\
\STATE{\textbf{Izlaz:} \\
	$x^*$ -- pronađena optimalna jedinka
}
\end{algorithmic}
\caption{Genetsko programiranje s tabu listom}
\label{alg:gp_taboo}
\end{algorithm}

\newpage

\subsection{Skup čvorova (prostor pretraživanja)}
\label{sec:node_set}
Skup čvorova koji se koriste pri pretraživanju definiraju prostor pretraživanja. Skup čvorova je implementiran tako da razlikuje čvorove različitih stupnjeva i omogućuje operatorima selektivni, nasumično selektivni ili potpuno nasumičan pristup. Početna vrijednost konstante pri dohvatu je postojana i definira se unaprijed, a vrijednost pojedine konstante u stablima mijenja se operatorima.

\begin{table}[H]
\centering
\begin{tabular}[t]{lr}
\begin{tabular}[t]{l|c|r}
\textbf{Naziv} & \textbf{Funkcija} & \textbf{Stupanj} \\
\hline
x		& $x$					& 0 \\
const	& $c \in \realnum$		& 0 \\
\hline
+		& $x + y$				& 2 \\
-		& $x - y$				& 2 \\
*		& $x \cdot y$			& 2 \\
/		& $\frac{x}{y + 10^{-12}}$	& 2 \\
\hline
min		& $min(x, y)$			& 2 \\
max		& $max(x, y)$			& 2 \\
abs		& $|x|$					& 1 \\
\hline
sin		& $sin(x)$				& 1 \\
cos		& $cos(x)$				& 1 \\
tan		& $tan(x)$				& 1 \\
\hline
exp		& $e^x$					& 1 \\
log		& $log_e(x)$				& 1 \\
pow2		& $x^2$					& 1 \\
pow3		& $x^3$					& 1 \\
pow		& $x^y$					& 2 \\
\end{tabular}
& \quad
\begin{tabular}[t]{l|c|r}
\textbf{Naziv} & \textbf{Funkcija} & \textbf{Stupanj} \\
\hline
gauss	& $e^{x^2}$		& 1 \\
relu		& \eqref{eq:relu}		& 1 \\
lrelu	& \eqref{eq:lrelu}		& 1 \\
threlu 	& \eqref{eq:threlu}		& 1 \\
sotplus	& \eqref{eq:softplus}	& 1 \\
elu		& \eqref{eq:elu}			& 1 \\
selu		& \eqref{eq:selu}		& 1 \\
swish	& \eqref{eq:swish}		& 1 \\
sigmoid	& \eqref{eq:sigmoid}		& 1 \\
hsigmoid & \eqref{eq:hsigmoid}	& 1 \\
tanh		& \eqref{eq:tanh}		& 1 \\
htanh 	& \eqref{eq:htanh}		& 1 \\
rattanh & \eqref{eq:rattanh}		& 1 \\
rectanh & \eqref{eq:rectanh}		& 1 \\
softsign	& \eqref{eq:softsign}	& 1 \\
trsin 	& \eqref{eq:trsin}		& 1 \\
softmax	& \eqref{eq:softmax}		& 1 \\
\end{tabular}
\end{tabular}
\caption{Popis čvorova korištenih pri pretrazi}
\end{table}


\subsection{Operatori križanja}
U nastavku opisani operatori križanja primaju dva roditelja, obavljaju križanje i vraćaju jedno dijete. Neki operatori istovremeno stvaraju dva djeteta, no vraćaju samo jedno nasumično odabrano. Ne obavlja se biranje djece prema dobroti zbog skupocjenosti postupka evaluacije, no preporučuje se zbog kvalitetnije pretrage.

\subsubsection{Zamijeni podstabla}
Operator zamjene podstabla odabire u svakom roditelju po jedan čvor i zamijeni ih. Pri zamjeni čvorovi zadržavaju svoju djecu i time su efektivno zamijenjena podstabla. Operator služi za miješanje genotipa s čuvanjem podstruktura koje se pokazuju dobrima. %Očekivano miješanje potpunog binarnog stabla dubine $n$ je $XX$ čvorova.

Rubni slučajevi su zamjena korijena stabla i zamjena listova. Pri zamjeni korijena jedno dijete postaje podstablo drugog roditelja. Pri zamjeni listova operator je ekvivalentan operatoru zamjene čvorova odnosno konstanti ako su obje konstante.

% Očekivani odmak od zadnjeg sloja -> Desmos: x-\left(\sum_{n=1}^xn\cdot\frac{2^{\left(n-1\right)}}{2^x-1}\ \right)

\subsubsection{Zamijeni čvorove}
Operator zamjene čvorova nasumično odabire čvor u stablu s manje čvorova. Zatim nasumično pronalazi čvor istog stupnja u većem stablu i zamjenjuje ih bez zamjene djece. Ako operator ne pronađe čvor istog stupnja, vraća nasumičnog roditelja. Prvo se odabire čvor iz manjeg stabla jer je veća vjerojatnost da će veće stablo sadržavati čvor istog stupnja, što je važno za unarne čvorove.

\subsubsection{Zamijeni konstante}
Operator zamjene konstanti nasumično odabire po jednu konstantu u oba roditelja i zamijeni ih. Operator je posebno koristan u kasnijem dijelu optimizacije kada pretraživanje funkcija konvergira, a pretražuju se optimalne konstante. U slučaju da barem jedan roditelj nema niti jednu konstantu, operator vraća nasumičnog roditelja.

\subsubsection{Usrednji konstante}
Operator usrednjavanja konstanti nasumično odabire po jednu konstantu u oba roditelja i jednu zamijeni njihovom aritmetičkom sredinom. Vraća se dijete sa zamijenjenom konstantom. Operator je posebno koristan u kasnijem dijelu optimizacije kada pretraživanje funkcija konvergira, a pretražuju se optimalne konstante. U slučaju da barem jedan roditelj nema niti jednu konstantu, operator vraća nasumičnog roditelja.

\subsubsection{Vrati nasumičnog roditelja}
Operator vraća nasumičnog roditelja kao dijete. Služi za osnaživanje konvergencije algoritma jer u populaciju vraća već postojeću jedinku koja dodatno prolazi kroz operator mutacije.


\subsection{Operatori mutacije}
U nastavku opisani operatori mutacije primaju jedno dijete i nad njime obave mutaciju. Operatori koji unose nove čvorove biraju čvorove iz skupa dostupnih čvorova \secref{sec:node_set}.

\subsubsection{Ubaci korijen}
Operator ubacivanja korijena stabla odabire nasumičan čvor stupnja većeg od 0 iz seta i postavlja ga korijenom stabla. Ako je odabrani čvor unaran, njegovo dijete postaje stari korijen. Ako je odabrani čvor većeg stupnja prvo dijete postaje korijen roditelja, a ostala njegova djeca popune se nasumičnim listovima iz skupa. Operator služi povećavanju dubine stabla i ključan je za stvaranje kompozicija funkcija.

\subsubsection{Ubaci list}
Operator ubacivanja lista odabire nasumičan čvor u stablu i zamijeni ga nasumičnim listom iz skupa. Ne postoje ograničenja na mjesto postavljanja lista pa operator može zamijeniti čitavo stablo listom. Iako je vrlo radikalan operator snažno pomaže očuvanju raznolikosti populacije. Operator služi smanjivanju dubine stabla i ključan je za pronalaženje plitkih stabala, što se odražava na brzinu izvođenja pronađenih funkcija.

\subsubsection{Postavi vrijednost konstante}
Operator odabire nasumičnu konstantu u stablu i dodjeljuje joj nasumičnu vrijednost. Vrijednost se uzorkuje iz uniformne distribucije zadanog intervala. Ako stablo ne sadrži konstante, operator ne mijenja stablo. Operator je ključan za pretraživanje prostora konstanti u kasnijem dijelu optimizacije kada pretraživanje funkcija konvergira.

\subsubsection{Postavi cjelobrojnu vrijednost konstante}
Operator je ekvivalentan operatoru postavljanja vrijednosti konstante, ali služi za pretraživanje prostora cjelobrojnih konstanti funkcija.

\subsubsection{Pribroji vrijednost konstanti}
Operator odabire nasumičnu konstantu u stablu i pribraja joj nasumičnu vrijednost. Vrijednost se uzorkuje iz normalne distribucije i skalira zadanom konstantom. Ako stablo ne sadrži konstante, operator ne mijenja stablo. Operator dijeli namjenu s operatorom postavljanja vrijednosti konstante.

\subsubsection{Izmjeni čvor}
Operator odabire nasumičan čvor u stablu i zamijeni ga nasumičnim čvorom iz seta istog stupnja. Stablo se neće promijeniti ako postoji samo jedan čvor tog stupnja u setu. Operator ne mijenja veličinu stabla te služi za postizanje blažih izmjena u genotipu. Treba obratiti pažnju da se izmjene genotipa mogu značajno odraziti na fenotip.

\subsubsection{Izmjeni podstablo}
Operator odabire nasumičan čvor i na njegovo mjesto postavlja generirano podstablo. Podstablo se generira zadanim inicijalizatorom. Operator unosi snažne izmjene genotipa i služi održavanju raznolikosti populacije i globalnoj pretrazi prostora. Rubni uvjet je kada se generira stablo dubine $1$, što utjecaj operatora čini identičnim operatoru ubacivanja lista.

\subsubsection{Ukloni korijen}
Operator iz korijena stabla odabire nasumično dijete i njega postavi kao novi korijen. Ako je korijen list, operator ne radi izmjene. Operator je komplementaran operatoru ubacivanja korijena te služi smanjivanju dubine stabla i ključan je za pronalaženje plitkih stabala.

\subsubsection{Ukloni unarni čvor}
Operator odabire nasumični unarni čvor i zamjenjuje ga njegovim djetetom. Ako stablo ne sadrži unarne čvorove operator ne unosi izmjene. Operator je posebno koristan pri eliminaciji dubokih kompozicija funkcija koje se znaju pojaviti pri pretraživanju, a koje nije moguće ukloniti operatorom uklanjanja korijena.

\subsubsection{Zamijeni redoslijed djece}
Operator odabire nasumičan čvor i njemu zamjeni mjesta dvaju djeteta. Ako je čvor stupnja $2$ operator će im zamijeniti mjesta. Ako je čvor stupnja većeg od $2$ operator će nasumično odabrati dva djeteta i njih zamijeniti. Pri tom operator može odabrati isto dijete dvaput čime se stablo ne mijenja. Za sve ostale čvorove operator ne unosi izmjene. Operator je koristan za promjenu utjecaja čvorova koji ovise o redoslijedu djece (npr. operator dijeljenja, oduzimanja, potenciranja i td.).

\subsubsection{Inicijaliziraj genotip}
Operator inicijalizacije genotipa zamjenjuje čitavo stablo novim generiranim stablom. Stablo se generira zadanim inicijalizatorom. Operator snažno pridonosi održavanju raznolikosti populacije i globalnoj pretrazi prostora.


%--------------------------------------------------------------------------------------%
\chapter{Implementacija}
\section{Razvojna okolina i alati}
Projekt je implementiran u programskom jeziku Java verzije 8, alatom IntelliJ. Razvojna okolina Deeplearning4j \citep{dl4j} korištena je za učitavanje podatkovnih skupova, pretprocesiranje, izgradnju i optimizaciju neuronskih mreža te podršku izvođenja na grafičkim karticama s podrškom CUDA biblioteke. Obrada podataka i iscrtavanje grafova funkcija i rezultata implementirano je Python skriptama i Jupyter bilježnicama u programskom jeziku Python verzije 3 korištenjem raznih biblioteka (numpy, sk-learn, matplotlib i ostalim). Slike neurona i gradijentnog spusta iscrtane su alatom \textit{draw.io}.

\noindent
Kod je dostupan na GitHub repozitoriju: \\
\url{https://github.com/lirfu/EvolvingActivationFunctions}

\section{Organizacija koda}
U implementaciji je napisano nekoliko izvršnih programa različitih namjena. Napisani su i \textit{JUnit} testovi za ispitivanje ispravnosti ključnih dijelova implementacije. U nastavku su spomenuti glavni dijelovi implementacije korisni korisniku za brže snalaženje.

\subsection{Evolucijski algoritmi}
Evolucijski algoritmi i pomoćni razredi implementirani su u paketu $genetics$. Dizajn biblioteke inspiriran je bibliotekom \textit{Evolutionary Computation Framework} \citep{ecf} u programskom jeziku C++. 

Najsloženiji razred je apstraktni razred $Algorithm$ koji čuva parametre i operatore algoritma, implementaciju inicijalizacije i pokretanja algoritma te praćenje i dohvat rezultata. Implementacije algoritma definiraju funkciju $runIteration$ u kojoj se obavljaju sve operacije pri jednoj iteraciji algoritma. Implementacije algoritma nalaze se u paketu $genetics.algorithms$. Algoritam podržava oblikovni obrazac \textit{graditelj} za jednostavniju i pregledniju definiciju algoritma. Primjere implementacije moguće je pronaći u paketu $genetics.algorithms$.

Genotip je definiran apstraktnim razredom $Genotype$ koji čuva svoju vrijednost dobrote i definira nekoliko apstraktnih metoda za rad s genotipom. Primjer implementacije genotipa je razred $symboregression.SymbolicTree$ opisan kasnije. Genotip je moguće serijalizirati u tekst.

Operatori su definirani apstraktnim razredom $Operator$ koji čuva vrijednost važnosti operatora te referencu na generator slučajnih brojeva. Operatori križanja $Crossover$ i mutacije $Mutation$ dodaju svoje apstraktne metode koje je potrebno implementirati i koje se pozivaju u algoritmu. Implementacije generičnih operatora nalaze se u korijenu paketa, dok se operatori specifični za pojedine vrste genotipa nalaze u zasebnim paketima tog genotipa (npr. $genetics.symboregression$). Parametre operatora moguće je serijalizirati u tekst.

Metoda odabira jedinki iz populacije definirana je sučeljem $Selector$ koja definira apstraktnu metodu $selectParentsFrom$ koja na ulaz dobiva populaciju, a vraća polje kandidata za roditelje. Ovisno o implementaciji, algoritam odabira može vratiti samo 2 roditelja ($RouletteWheelSelector$) ili 2 roditelja i jedinku koja se zamjenjuje djetetom ($TournamentSelector$). Metode odabira implementirane su u paketu $selectors$.

Inicijalizator populacije definiran je sučeljem $Initializer$. Uvjet zaustavljanja algoritma definiran je razredom $stopconditions.StopCondition$ čiji objekt se predaje algoritmu. Uvijete zaustavljanja moguće serijalizirati u tekst. Rezultat algoritma opisan je razredom $Result$ kojeg je moguće serijalizirati u tekst.

\subsubsection{Simbolička regresija}
Implementacija simboličke regresije nalazi se u paketu $genetics.symboregression$. Sadrži operatore križanja definirane paketom $crx$ i operatore mutacije definirane paketom $mut$. Genotip je definiran razredom $SymbolicTree$ koji sadrži referencu na korijenski čvor. Čvorovi su definirani apstraktnim razredom $TreeNode$, a skup dostupnih čvorova s pristupnim metodama definiran je razredom $TreeNodeSet$.

Izvršavanje operacija obilaskom stabla obavlja se objektom sučelja $IExecutable$ koji se definira pri implementaciji čvora. Čvor sadrži i operacije za dohvat i zamjenu djece te izvršnog objekta, a koje su potrebne operatorima križanja i mutacije. Čvor može primiti i dodatan objekt koji može poslužiti čvorovima specijalne namjene.

\subsubsection{Evaluator}
S obzirom da je postupak treniranja i validacije često vrlo zahtjevan postupak u evaluator je ugrađena memorija evaluiranih aktivacijskih funkcija. Jedinka se prvo serijalizira u tekstualni oblik i uspoređuje s memorijom. Ako je pronađena, dohvaća se njena vrijednost i vraća umjesto ponovljenog treniranja. Postupak je valjan samo ako su eksperimenti ponovljivi za zadane parametre što u ovom projektu vrijedi.

\subsection{Neuronska mreža}
Neuronska mreža definirana je razredom $CommonModel$ te služi za transparentnu izgradnju mreže iz zadanih parametara, čuvanje instance mreže i povijesti vrijednosti gubitka nakon učenja.

\subsubsection{Zajednički razred za optimizaciju mreža}
Za potrebe transparentnog učenja, validacije i testiranja neuronskih mreža te ponovljivost eksperimenata i memorijske uštede definiran je razred $TrainProcedureDL4J$. Pri inicijalizaciji razred učitava skupove za treniranje i testiranje. Ako je tako definirano parametrima, značajke skupova se normaliziraju prema skupu za učenje te se, po potrebi, iz izmiješanog skupa za učenje gradi skup za validaciju. Miješanje skupa za učenje je konzistentno u svim eksperimentima.
Razred nasljeđuje sučelje s istim metodama kako bi se omogućila drugačija implementacija razreda. Jedan primjer je mogućnost pozivanja skripti koje implementiraju te postupke. Za komunikaciju s Python skriptama izgrađen je pomoćni razred $PythonBridge$.

\subsection{Pomoćni mehanizmi projekta}

\subsubsection{Standardizirana pohrana rezultata}
Zbog potrebe standardizirane i transparentne pohrane rezultata i parametara eksperimenata definiran je razred $StorageManager$ koji definira zapisivanje podataka i putanje datoteka za različite eksperimente. Opisnik eksperimenta definiran je razredom $Context$, a sadrži naziv podatkovnog skupa i naziv eksperimenta. Metode za pohranu primaju podatke koji se zapisuju i opisnik kojim se izgradi putanja (npr. za Windows sustave: $podatkovni\_skup \textbackslash eksperiment$). Poddirektoriji datoteke grade se automatski, a putanje se grade razdjelnikom ovisnom o operacijskom sustavu (definiran u Javi: $File.separator$). Standardizirana pohrana rezultata uvelike olakšava kasniju obradu podataka.

\subsubsection{Hiperparametri}
Hiperparametre programa moguće je zadati konfiguracijskom datotekom. Datoteka je formata ključ-vrijednost, odvojeni regularnim izrazom: "$[:\ \textbackslash t]+$". Kroz datoteku moguće je definirati i više vrijednosti parametara koji će se pretraživati po rešetci (npr. $\{p1,p2,p3\}$). Datotekom je moguće zadati sve parametre postupaka, uključujući sjeme korišteno u brojnim procesima. Ako nije zadan važan parametar program će baciti iznimku.

Hiperparametri za učenje neuronskih mreža su definirani razredom $TrainParams$ i njima se služi razred za optimizaciju mreža. Hiperparametri za evoluciju aktivacijskih funkcija definirani su razredom $EvolvingActivationParams$ koji naslijeđuje $TrainParams$ te kojim se služi izvršni program $EvolvingActivationProgram$.

\subsubsection{Paralelizacija}
Paralelizaciju je moguće postići vrlo jednostavno paketom $utils.threading$. Prvo je potrebno instancirati razred $WorkArbiter$ koji u sebi sadrži red poslova koje treba obaviti i listu radnika. Posao koji treba izvršiti definira se implementacijom sučelja $Work$. Poslovi se predaju objektu $WorkArbiter$ koja ih stavlja u red. Radnici $Worker$ paralelno i sinkronizirano dohvaćaju i izvršavaju poslove iz reda poslova razreda $WorkArbiter$ koji ih je stvorio. S obzirom da su radnici implementirani kao zasebne dretve, navalom poslova radnici će paralelno uzimati poslove i izvršavati ih.

Čekanje pozivajuće dretve na izvršenje zadataka moguće je ostvariti pozivom metode $waitOn$ koja prima uvjet čekanja $WaitCondition$. Moguće je definirati svoj uvjet (npr. čekanje dok se ne popuni spremnik) ili pričekati dok se svi poslovi ne izvrše, uvjetom koji vraća metoda $getAllFinishedCondition$.

Pri definiranju poslova često je potrebno definirati lokalnu varijablu s modifikatorom $final$ koja se ažurira po početku ili završetku posla. Kako se ne bi trebalo definirati polje (što je česta praksa) definiran je pomoćan generički razred $Holder$ koji nudi metode za dohvat i postavljanje unutarnjeg objekta. Za potrebe nabrajanja definiran je razred $Counter$, a za zajedničko pamćenje dva ili tri objekta definirani su generički razredi $Pair$ i $Triple$.

\subsubsection{Bilježenje napretka algoritma}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
\includegraphics[width=\textwidth]{pkg_logs.pdf}
\centering
\caption{Paket $utils.logs$}
\label{fig:pkg_logs}
\end{figure}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Pri izvršavanju algoritma učenja ili pretrage predaje se objekt koji implementira sučelje $ILogger$. Objekt nudi metode za selektivno bilježenje informacija o izvođenju programa ili algoritma ovisno o njihovom tipu. Metode selektivnog bilježenja inspirirane su objektom $Log$ Java razvojne biblioteke operacijskog sustava Android. Postoji nekoliko izvedbi koje preusmjeravaju ulaze ostalim mehanizmima. Tablica \ref{tab:loggers} prikazuje implementacije i njihovo odredište.

\begin{table}[h]
\begin{tabular}{ll}
Klasa & Odredište \\
\hline
DevNullLogger & Ignorira ulaze \\
StdoutLogger & Standardni izlaz \\
FileLogger & Zapisuje u zadanu datoteku (s kreiranjem poddirektorija) \\
SlackLogger & Kanal servisa Slack \\
MultiLogger & Prosljeđuje listi zadanih implementacija
\end{tabular}
\centering
\caption{Implementacije za bilježenje}
\label{tab:loggers}
\end{table}

\subsection{Izvršni programi projekta}
U projektu su napisani brojni izvršni programi čije namjene uključuju demonstraciju dodataka, izvršavanje eksperimenata, ponavljanje eksperimenta s dodatnim bilježenjem i slično. U nastavku su opisani programi korišteni za izvršavanje eksperimenata.

\subsubsection{Pohlepna pretraga uobičajenih funkcija}
Razred ovog programa je $GreedySearchProgram$. Program služi za pohlepnu pretragu opisanu u poglavlju \ref{sec:greedy_dpav4} te za eksperimentiranje kombinacijama hiperparametara. Ne prima ulaze već je predodređen za aktivno mijenjanje koda. Na početku programa se nalaze zajednički hiperparametri te su definirane funkcije i arhitekture čije kombinacije želimo ispitati. Sadrži i mehanizam nastavljanja eksperimenata ako je došlo do prekida. Pod nastavljanjem se misli na nastavak pretraživanja po rešetci, samo učenje mreže nije moguće nastaviti. Na dnu programa nalaze se vrijednosti hiperparametara koje se pretražuju za svaku kombinaciju funkcije i arhitekture.

\subsubsection{Izgradnja aktivacijskih funkcija simboličkom regresijom}
Razred ovog programa je $EvolvingActivationProgram$. Program služi pokretanju eksperimenata s evolucijom aktivacijske funkcije. Osnovni ulaz u program je putanja do datoteke s hiperparametrima algoritma. Drugi ulaz je putanja do .jar datoteke tog istog programa i nije nužan. Njime se program prebacuje u procesni način rada, pri čemu se svaki eksperiment pokreće u zasebnom procesu. Ovime se prisiljava oslobađanje memorije prethodnog eksperimenta koja može procuriti iz nepoznatih razloga. Gašenjem glavnog procesa automatski se gasi i proces djete.

Prilikom izvršavanja može doći do velikih zahtjeva za memorijom. Ako nisu postavljene gornje granice memorijskih zahtjeva, doći će do prekida programa. Za povećavanje gornje memorijske granice JVM-a, prilikom pokretanja JVM-u se treba zadati zastavica, npr. $-Xmx4g$ koja postavlja granicu na 4GB. Za ograničavanje memorije koju DL4j zauzima izvan gomile i na grafičkim karticama, treba se zadati zastavica, npr. $-Dorg.bytedeco.javacpp.maxbytes=3g$ koja postavlja granicu na 3GB.

%--------------------------------------------------------------------------------------%
\chapter{Rezultati}
\label{sec:rezultati}

\section{Usporedba uobičajenih aktivacijskih funkcija}
U ovom poglavlju prikazana je usporedba performansi popularnih aktivacijskih funkcija. Ispitivane su kombinacije arhitektura i aktivacijskih funkcija te su priložene postignute mjere točnosti i F1 na skupu za testiranje.

\begin{table}[H]
\centering
\begin{tabular}{lr}
Hiperparametar & Vrijednosti \\
\hline
Sjeme inicijalizacije & 42 \\
Veličina minigrupe & 256 \\
Normalizacija značajki & Da \\
Permutacija mini-grupa & Ne \\
Normalizacija mini-grupe & Da \\
Dropout & Ne \\
Koeficijent opadanja stope učenja & 0.99 \\
Broj iteracija do idućeg opadanja stope učenja & 1 \\
Maksimalan broj epoha & 40 \\
Broj uzastopnih iteracija za rano zaustavljanje & 5 \\
Minimalna relativna promjena gubitka za detekciju konvergencije & 0.01 \\
\hline
Koeficijent L2 regularizacije & $10^{-3}$, $10^{-4}$, $10^{-5}$ \\
Stopa učenja & $10^{-3}$, $5 \cdot 10^{-4}$, $10^{-4}$
\end{tabular}
\caption{Hiperparametri korišteni pri učenju mreža. Zarezima su odvojene vrijednosti hiperparametara koje su pretraživane po rešetci.}
\label{tab:hp_common}
\end{table}

Za svaku kombinaciju arhitekture i aktivacijske funkcije provedeno je pretraživanje hiperparametara po rešetci s usporedbom na skupu za validaciju. Skup za validaciju nastao je nasumičnim djeljenjem skupa za treniranje u omjeru $80:20$, a podjela skupa je konzistentna u svim mjerenjima u ovom radu. Zajednički i pretraživani hiperparametri navedeni su tablici \ref{tab:hp_common}. Kombinacije hiperparametara s najboljim postignućem navedene su u dodatku \ref{app:grid_hp}. Navedeni su najbolji postignuti rezultati na netaknutom skupu za testiranje.

\subsection{DPAv2}

\begin{figure}[H]
\includegraphics[width=\textwidth]{greedy_256_acc.pdf}
\centering
\caption{Postignuta točnost aktivacijskih funkcija na različitim arhitekturama. Vrijednosti u ćelijama su tri najznačajnije znamenke nakon decimalne točke. Na vrijednosti mjere $0.5$ tekst mijenja boju iz bijele u crnu.}
\label{fig:greedy_256_acc}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{greedy_256_f1.pdf}
\centering
\caption{Postignuta F1 mjera aktivacijskih funkcija na različitim arhitekturama. Vrijednosti u ćelijama su tri najznačajnije znamenke nakon decimalne točke. Na vrijednosti mjere $0.5$ tekst mijenja boju iz bijele u crnu.}
\label{fig:greedy_256_f1}
\end{figure}

Pretragom na skupu DPAv2 pojavljuju se jasne razlike između funkcija i između arhitektura. Na dvoslojnim arhitekturama najboljima se pokazuju sinus, kosinus i sigmoida, na troslojnim sinus, swish i sigmoida, a na četveroslojnim swish, sinus i sigmoida. Funkcijama exp, pow3, softmax, softplus i threlu mreža konzistentno nije uspješno naučena. 

\iffalse
\todoimg{mjesta aktivacije top 4 funkcije po arh}
\todo{Komentar}

\todoimg{mjesta aktivacije bottom 4 funkcije po arh}
\todo{Komentar}

Zanimljivost se pojavljuje kod funkcije pow2 na prijelazu iz dvoslojnih u troslojne arhitekture. Odjednom mrežu više nije moguće naučiti.
\todo{analiza slučaja pow2}
\fi

\begin{figure}[ht]
\includegraphics[width=\textwidth]{greedy_256_arch_quality.pdf}
\centering
\caption{Distribucija F1 mjere po svim arhitekturama}
\label{fig:greedy_256_arch_quality}
\end{figure}

Arhitekture $[50-50-50]$ i $[50-50-50-50]$ imaju najlošije rezultate po svim funkcijama što upućuje na manjak njihove reprezentativne moći. Usporedbom rezultata po arhitekturama \figref{fig:greedy_256_arch_quality} najveću ostvarenu vrijednost i najveću srednju vrijednost ostvaruje arhitektura [500-500], no zbog velike raspršenosti rezultata oko sredine i memorijske zahtjevnosti ta arhitektura nije odabrana. U daljnjim eksperimentima koristi se arhitektura \textbf{[300-300]} jer ostvaruje drugu najveću srednju vrijednost i najmanju raspršenost. Koristi se najčešća kombinacija hiperparametara za tu arhitekturu: koeficijent L2 regularizacije je $10^{-4}$, a stopa učenja je $5\cdot 10^{-4}$.

\clearpage

\subsection{DPAv4}
\label{sec:greedy_dpav4}

\begin{figure}[H]
\includegraphics[width=\textwidth]{greedy_9_acc.pdf}
\centering
\caption{Postignuta točnost aktivacijskih funkcija na različitim arhitekturama. Vrijednosti u ćelijama su tri najznačajnije znamenke nakon decimalne točke. Na vrijednosti mjere $0.5$ tekst mijenja boju iz bijele u crnu.}
\label{fig:greedy_9_acc}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{greedy_9_f1.pdf}
\centering
\caption{Postignuta F1 mjera aktivacijskih funkcija na različitim arhitekturama. Vrijednosti u ćelijama su tri najznačajnije znamenke nakon decimalne točke. Na vrijednosti mjere $0.5$ tekst mijenja boju iz bijele u crnu.}
\label{fig:greedy_9_f1}
\end{figure}

Usporedbom točnosti sve kombinacije su više-manje kompetitivne, no usporedom F1 mjere razlike su snažnije istaknute. Ponovno se najboljim pokazuje sinusoida, a na dvoslojnim arhitekturama i kosinus. Funkcija $exp$ i $pow3$ mreža konstantno nije naučila, a ponovno se pojavljuje nemogućnost treniranja troslojnih arhitektura $pow2$ funkcijom.

\begin{figure}[H]
\includegraphics[width=\textwidth]{greedy_9_arch_quality.pdf}
\centering
\caption{Distribucija F1 mjera po svim arhitekturama}
\label{fig:greedy_9_arch_quality}
\end{figure}

Usporedbom rezultata po arhitekturama \figref{fig:greedy_9_arch_quality} najviše ostvarene i srednje vrijednosti postižu dvoslojne arhitekture, no za neke odabire aktivacijski funkcija ne postižu zadovoljavajuće rezultate. To upućuje na manjkavost reprezentacijske moći i osjetljivost postupka učenja tih arhitektura. Najmanju raspršenost po funkcijama oko srednje vrijednosti i među najvišim srednjim vrijednostima ostvaruje arhitektura \textbf{[50-50-50]} što ju čini povoljnom za daljnje eksperimente. Koristi se najčešća kombinacija hiperparametara za tu arhitekturu: koeficijent L2 regularizacije je $10^{-4}$, a stopa učenja je $5\cdot 10^{-4}$.

\clearpage

\section{Izgradnja aktivacijskih funkcija genetskim \\ programiranjem s tabu listom}
U ovom poglavlju prikazani rezultati ostvareni izgradnjom aktivacijskih funkcija genetskim programiranjem. Za svaki eksperiment odabrani su nepromjenjivi hiperparametri i arhitektura iz prethodnog poglavlja i mijenjana je jedino aktivacijska funkcija. Jedina razlika: zbog traktabilnosti postupka \textbf{broj epoha učenja je smanjen na 30}. Rezultati su uspoređeni s rezultatima identičnih postava iz prethodnog poglavlja. 

\begin{table}[H]
\centering
\begin{tabular}{lr}
Hiperparametar & Vrijednosti \\
\hline
Veličina populacije & 12 \\
Broj iteracija & 30 \\
Elitizam & Da \\
Vjerojatnost mutacije & 0.3 \\
Mehanizam odabira & kotač ruleta \\
Funkcija dobrote & negativna F1 mjera \\
Broj iteracija tabu diverzifikacije & 1000 \\
\hline
\multirow{5}{*}{Križanja}
& Zamijeni podstabla \\
& Zamijeni čvorove \\
& Zamijeni konstante \\
& Usrednji konstante \\
& Vrati nasumičnog roditelja \\
\hline
\multirow{7}{*}{Mutacije}
& Ubaci korijen \\
& Ubaci list \\
& Postavi konstantu iz $[-1,1]$ \\
& Izmjeni čvor \\
& Ukloni korijen \\
& Ukloni unarni čvor \\
& Zamijeni redoslijed djece \\
\end{tabular}
\caption{Hiperparametri korišteni pri evoluciji aktivacijske funkcije}
\label{tab:gp_hiperparams}
\end{table}

Parametri evolucije su zajednički svim eksperimentima i dani su tablicom \ref{tab:gp_hiperparams}. Vrijednost veličine tabu liste je mijenjana po broju populacija (veličina je broj zapamćenih populacija pomnožen s brojem jedinki u populaciji). Za svaku veličinu tabu liste provedeno je 10 eksperimenata s različitim sjemenjima nasumičnog generatora.

\subsection{DPAv2}
Na DPAv2 podatkovnom skupu pronađene su aktivacijske funkcije koje poboljšavaju generalizacijsku moć mreže u odnosu na popularne funkcije. Najbolja pronađena funkcija daje rezultate ekvivalentne najboljem rezultatu prethodnih eksperimenata za mrežu sa znatno više parametara ([500-500]). Najbolje pronađene funkcije (tablica \ref{tab:dpav2_top5}) sadrže periodičku komponentu (sinus ili kosinus).

\begin{table}[H]
\begin{tabular}{lrrr}
Funkcija & Točnost & F1 mjera & Taboo veličina \\
\hline
\figref{fig:dpav2_gp_f1} & 0.587 & 0.582 & 2 \\
\figref{fig:dpav2_gp_f2} & 0.585 & 0.581 & 3 \\
\figref{fig:dpav2_gp_f3} & 0.585 & 0.580 & 0 \\
\figref{fig:dpav2_gp_f4} & 0.585 & 0.579 & 0 \\
\figref{fig:dpav2_gp_f5} & 0.582 & 0.577 & 3 \\
\end{tabular}
\centering
\caption{Najboljih 5 pronađenih aktivacijskih funkcija}
\label{tab:dpav2_top5}
\end{table}

\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav2_gp_f1.pdf}
\centering
\caption{$(\cos(x-4.153574932708071))^2$}
\label{fig:dpav2_gp_f1}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav2_gp_f2.pdf}
\centering
\caption{$elu (elu (\min (1.0,\min (1.0,hsigm (x)) \cdot 0.8102254104210314) \cdot \sin (x)))$}
\label{fig:dpav2_gp_f2}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav2_gp_f3.pdf}
\centering
\caption{$(\cos(1.0 + x))^2$}
\label{fig:dpav2_gp_f3}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav2_gp_f4.pdf}
\centering
\caption{$-swish(swish(\cos(x - 0.8949527515835601))) \cdot x$}
\label{fig:dpav2_gp_f4}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav2_gp_f5.pdf}
\centering
\caption{$swish (elu (\sin (\cos (x - 1.0 + 0.1607274601962161^9))))$}
\label{fig:dpav2_gp_f5}
\end{figure}

U odnosu na najbolji rezultat prethodnih eksperimenata za korištenu arhitekturu, postupak je pronašao bolje aktivacijske funkcije, no vrijednost poboljšanja je vrlo niska (za F1 mjeru je oko $0.01$). Povećavanjem tabu liste algoritam pokazuje se trend porasta prosječne kvalitete rješenja. U odnosu na klasično genetsko programiranje (tabu veličina 0), tabu lista poboljšava i stabilnost postupka s obzirom na pronađene kvalitete rješenja (raspršenost je manja).

\begin{figure}[H]
\includegraphics[width=.9\textwidth]{GP_256class_acc_plus.pdf}
\centering
\caption{Točnosti po veličini tabu liste}
\label{fig:gp_256_acc}
\end{figure}

\begin{figure}[H]
\includegraphics[width=.9\textwidth]{GP_256class_f1_plus.pdf}
\centering
\caption{F1 mjera po veličini tabu liste}
\label{fig:gp_256_f1}
\end{figure}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Izgradnja heterogenog rasporeda aktivacijskih funkcija}
\todo{Ne znam hoću li stići i ovo izvrtiti. Za najbolju prethodnu arhitekturu ću pronaći najbolji raspored uobičajenih fja po slojevima mreže (npr. sin-relu).}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{DPAv4}
Na DPAv4 podatkovnom skupu najbolje pronađene aktivacijske funkcije daju rezultate ekvivalentne najboljem rezultatu prethodnih eksperimenata. Najbolje pronađene funkcije spomenute su u tablici \ref{tab:dpav4_top5}.

\begin{table}[H]
\begin{tabular}{lrrr}
Funkcija & Točnost & F1 mjera & Taboo veličina \\
\hline
\figref{fig:dpav4_gp_f1} & 0.966 & 0.952 & 3 \\
\figref{fig:dpav4_gp_f2} & 0.966 & 0.950 & 1 \\
\figref{fig:dpav4_gp_f3} & 0.966 & 0.948 & 2 \\
\figref{fig:dpav4_gp_f4} & 0.966 & 0.948 & 3 \\
\figref{fig:dpav4_gp_f5} & 0.962 & 0.947 & 4 \\
\end{tabular}
\centering
\caption{Najboljih 5 pronađenih aktivacijskih funkcija}
\label{tab:dpav4_top5}
\end{table}

\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav4_gp_f1.pdf}
\centering
\caption{$\sin (\min (\sin (\min (x,0.5068783911631836) + 1.0),-0.9990098031722499) + x)$}
\label{fig:dpav4_gp_f1}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav4_gp_f2.pdf}
\centering
\caption{$\cos (x) \cdot 0.9910098031767487$}
\label{fig:dpav4_gp_f2}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav4_gp_f3.pdf}
\centering
\caption{$ReLU (abs (x) \cdot x)$}
\label{fig:dpav4_gp_f3}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav4_gp_f4.pdf}
\centering
\caption{$\cos (x) \cdot 0.9815578450294762$}
\label{fig:dpav4_gp_f4}
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{dpav4_gp_f5.pdf}
\centering
\caption{$\frac{x - \exp(x - 1.0)}{1.0}$}
\label{fig:dpav4_gp_f5}
\end{figure}

U odnosu na najbolji rezultat prethodnih eksperimenata za korištenu arhitekturu, postupak je povećao kvalitetu mreže, ali za vrlo malu vrijednost (oko $0.01$ za F1 mjeru) i povećanje nije konzistentno za za sve veličine tabu liste. Povećavanjem veličine tabu liste razlike u kvaliteti rješenja su vrlo male i raspršenost rezultata varira.

\begin{figure}[H]
\includegraphics[width=.9\textwidth]{GP_9class_acc_plus.pdf}
\centering
\caption{Točnosti po veličini tabu liste}
\label{fig:gp_9_acc}
\end{figure}

\begin{figure}[H]
\includegraphics[width=.9\textwidth]{GP_9class_f1_plus.pdf}
\centering
\caption{F1 mjera po veličini tabu liste}
\label{fig:gp_9_f1}
\end{figure}

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Izgradnja heterogenog rasporeda aktivacijskih funkcija}
\todo{Ne znam hoću li stići i ovo izvrtiti. Za najbolju prethodnu arhitekturu ću pronaći najbolji raspored uobičajenih fja po slojevima mreže (npr. sin-relu).}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%--------------------------------------------------------------------------------------%
\chapter{Isprobane metode koje nisu korištene}
U nastavku su navedene metode koje su isprobane, ali nisu korištene u konačnom rješenju.

\subsubsection{Učeći parametri}
Neke aktivacijske funkcije uvode dodatne učeće parametre kako bi poboljšale stabilnost učenja i konačne performanse \citep{prelu, apl, network_in_network}. Ti parametri povećavaju kapacitet mreže i ostvaruju heterogenu mrežu kojoj svaki neuron prilagođava aktivacijsku sebi.

Iako ih je moguće implementirati kao čvor stabla, u ovom se radu ne koriste učeći parametri zbog kompleksnosti implementacije i umjesto njih se pri genetskom programiranju koriste isključivo konstante.

\subsubsection{Permutacija primjera između grupa}
Permutacija primjera između grupa primjenjuje se između epoha učenja neuronske mreže tako da se prije izgradnje minigrupa čitav skup permutira. Postupak ima regularizacijski efekt i pospješuje rezultate. Pri stohastičkom gradijentnom spustu stohastičnost gradijenata je naglašena i poboljšana je otpornost na lokalne optimume.

U inicijalnim eksperimentima, permutacija grupa je pokazala poboljšanje F1 mjere i do $0.06$ na DPAv4 skupu. No, postupak je vremenski i memorijski zahtjevan u trenutnoj paralelnoj implementaciji jer pri svakom početku učenja zahtjeva stvaranje kopije čitavog skupa. Postupak je vrlo jednostavno moguće ostvariti ako se implementira poseban iterator podataka sa stohastičkim uzorkovanjem, no nije implementiran zbog vremenskih ograničenja projekta i slabo dokumentiranih mehanizama biblioteke DL4j.

\subsubsection{Dropout}
Dropout \citep{dropout} je tehnika regularizacije neuronskih mreža koja ostvaruje dobre rezultate na vrlo dubokim i širokim modelima koji su skloni pretreniranju i koadaptaciji neurona. Radi tako da za vrijeme učenja mreže proporcionalno zadanoj vjerojatnosti nasumično isključuje neurone u oba smjera. Tehnika efikasno sprječava koadaptaciju neurona širokih slojeva jer je vjerojatnost da će dva neurona koji su skloni koadaptaciji biti identično naučena vrlo mala. Isključivani neuroni će ipak učiti jer vjerojatnost da će neuron ostati isključen kroz više iteracija obrnuto proporcionalna broju iteracija.

Negativna posljedica Dropout-a je duže vrijeme učenja jer je potrebno više iteracija da svaki neuron dobije dovoljan broj efektivnih iteracija učenja. Ta činjenica nije korisna za postupke neuroevolucije kojima trošak evaluacije definira traktabilnost postupka. U sklopu projekta, provedeni su i preliminarni eksperimenti koji su dokazali spomenutu neučinkovitost.

\subsubsection{TensorFlow Java API}
Na početku projekta isproban je TensorFlow API za Javu i napisan je mali demonstracijski program. Iako program radi, usporedba postupka učenja i rezultata s identičnim programom u Python-u pokazuje razlike. Dodatno, kako u trenutku pisanja rada ovaj API još uvijek nije pod garancijom stabilnosti i nedostaju implementacije ključnih elemenata, odustalo se od rada s tom bibliotekom.

\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Buduća istraživanja}
\todo{Ispitivanje učinkovitosti korištene optimizacije na ostalim zadatcima}
\todo{Operatori različitog utjecaja i uzorkovanje s rastućim pragom osjetljivosti na izmjene populacije. Operatori imaju definiranu snagu, kako vrijeme ide sve manje se biraju (putujuća sigmoida) snažni operatori -> konvergencija kao sim. kaljenje}
\todo{Paralelna evolucija arhitekture i aktivacijskih fja \citep{cnn_evolution}}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Zaključak}
Potpuno povezane umjetne neuronske mreže korištene su za klasifikacijski zadatak predviđanja kriptografskog ključa iz naponskih mjerenja kriptografskog uređaja. Za mjerenja su korišteni podatkovni skupovi DPAv2 i DPAv4 s reduciranim brojem ulaznih značajki. Performanse mreža uspoređene su na nekoliko odabira arhitekture i aktivacijske funkcije te je korišten postupak izgradnje aktivacijske funkcije genetskim programiranjem.

Pokazuje se da odabir aktivacijske funkcije utječe na performanse neuronske mreže. Najboljom funkcijom pokazala se sinusoida, rijetko korištena u modernoj literaturi dubokog učenja. Izgradnjom aktivacijskih pronađena su mala poboljšanja u svojstvu generalizacije mreže.

Uvođenjem tabu liste u algoritam genetskog programiranja moguće je poboljšati kvalitetu pronađenih rješenja i usmjeriti postupak pretrage prema neistraženim rješenjima.

\bibliography{literatura}
\bibliographystyle{fer}

\begin{sazetak}
Proučiti postojeće metode u izgradnji aktivacijskih funkcija u umjetnim neuronskim mrežama. Posebnu pažnju posvetiti evolucijskim algoritmima simboličke regresije za izgradnju ciljanih funkcija. Ustanoviti moguće nedostatke postojećih algoritama ili mogućnost poboljšanja. Primijeniti evoluirane aktivacijske funkcije u homogenoj ili heterogenoj umjetnoj neuronskoj mreži na skupovima DPAv2 i DPAv4 te odrediti mjere kvalitete izgrađenog klasifikatora: točnost, preciznost, odziv te F mjere. Usporediti učinkovitost ostvarenih postupaka s postojećim rješenjima iz literature. Radu priložiti izvorne tekstove programa, dobivene rezultate uz potrebna objašnjenja i korištenu literaturu.

\kljucnerijeci{AES,SCA,DPA,kriptografija,umjetna neuronska mreža,aktivacijska funkcija,neuroevolucija,genetsko programiranje,tabu lista}
\end{sazetak}

\engtitle{Optimized activation functions for classifiers based on artificial neural networks in the domain of implementation attacks on cryptographic devices}
\begin{abstract}
Examine existing methods in building activation functions in artificial neural networks. Give special attention to evolutionary algorithms of symbolic regression for constructing the targeted functions. Apply evolved activation functions in a homogeneous or heterogeneous artificial neural network on datasets DPAv2 and DPAv4 and examine quality measures of the built classifier: accuracy, precision, recall and F measures. Compare the efficiency of acquired methods with existing solutions from the literature. Alongside thesis attach source code of programs, acquired results with necessarry discussion and literature used.

\keywords{AES,SCA,DPA,cryptography,artificial neural network,activation function, neuroevolution,genetic programming,taboo list}
\end{abstract}

\appendix
\chapter{Hiperparametri pretrage po rešetci}
\label{app:grid_hp}

\begin{table}[H]
\centering
\begin{tabular}{lcc}
Indeks & Koeficijent L2 reg. & Stopa učenja \\
\hline
1 & $10^{-3}$ & $10^{-3}$  \\
2 & $10^{-3}$ & $5 \cdot 10^{-4}$ \\
3 & $10^{-3}$ & $10^{-4}$ \\
4 & $10^{-4}$ & $10^{-3}$ \\
5 & $10^{-4}$ & $5 \cdot 10^{-4}$ \\
6 & $10^{-4}$ & $10^{-4}$ \\
7 & $10^{-5}$ & $10^{-3}$ \\
8 & $10^{-5}$ & $5 \cdot 10^{-4}$ \\
9 & $10^{-5}$ & $10^{-4}$
\end{tabular}
\caption{Indeksi kombinacija hiperparametara korištenih pri pretraživanju po rešetci.}
\label{tab:hp_comb}
\end{table}

\begin{figure}[H]
\includegraphics[width=\textwidth]{greedy_9_hp.pdf}
\centering
\caption{Skup \textbf{DPAv4}: Optimalne kombinacije hiperparametara za aktivacijske funkcije na različitim arhitekturama. Vrijednosti u ćelijama označavaju kombinacije hiperparametara navedene u tablici \ref{tab:hp_comb}}
\label{fig:greedy_9_hp}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{greedy_256_hp.pdf}
\centering
\caption{Skup \textbf{DPAv2}: Optimalne kombinacije hiperparametara za aktivacijske funkcije na različitim arhitekturama. Vrijednosti u ćelijama označavaju kombinacije hiperparametara navedene u tablici \ref{tab:hp_comb}}
\label{fig:greedy_256_hp}
\end{figure}

\end{document}
